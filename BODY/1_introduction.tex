%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Introduction ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{chap:intro}
\pagestyle{fancy}

Music is a cornerstone in modern video games. Music evokes emotional responses, triggers memories, can direct attention and improve immersion and the overall experience of a game. Yet, it is not uncommon for players to become disengaged from the game music, often due to excessive repetition or simply different personal preferences.\cite{Rogers_Weber_2019} This can be a problem in “serious” games for therapeutic use such as “Last Minute Gig” a gamified Musical Attention Control Training (MACT) application, aimed to improve attention control in patients with Parkinson’s disease \cite{Chalkiadakis_2022}. Ideally MACT is a personalized experience with dynamic adjustments taking into account a patient's abilities and preferences. In the context of gamified self-directed therapy,this is crucial to keep patients engaged and progressing.\footnote{This type of therapy is not aimed at replacing traditional guided therapy. Ideally it is used as supplement, and may provide relief in situations where traditional therapy is not immediatly available} In order to accommodate a wide array of patient's abilities and preferences in a MACT-game, a considerable amount of musical, specifically musical material that fits the constraints of MACT is required. One potential avenue to provide this is through controlled music generation.

Over the last years, music generation has grown from a mostly academic endeavor to a billion-dollar industry. Large technology companies are exploring music generation with foundation models such as Meta’s MusicGen\cite{copet2023simple} or Google’s MusicLM 
\cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023}. The commercial startup Suno is valued at 500 million dollars, while chart-topping songs are being generated with Udio \cite{Ferdinand_Meyen_2024}\cite{Stassen_2024}. This breakthrough is powered mostly by advances in language modelling, bringing about Large Language Models such as GPT-4 or LLAMA3 applied to music.

In the context of games, generative music is promising to cater to different user preferences and enable a richer, more varied musical experience. Video games can include hundreds of hours of game-play and branching story lines. When additional complexity is added to adapt the music to individual preferences, it quickly is no longer feasible to manually compose unique music to fit all scenarios. Especially in more resource constrained applications such as games for therapeutic use, generated music can offer an avenue to provide a large variety of music that can improve user engagement, and ultimately the effect of the intervention.
One of the key components of a successful generative model is control. In music generation control could be a text-description, information about genre, style or instrumentation. Control can also include fine-grained musical parameters such as a melody, chord progression or musical structure. Generative models can often be controlled with human-composed music, where the provided music is continued, interpolated between or inpainted (generating accompaniments, melodies, or additional musical lines). In LastMinuteGig\cite{Chalkiadakis_2022} the music periodically provides stimuli, a noticeable change in the music, based on which the user changes their playing, specifically their rhythm. One way this stimulus could be achieved in generated music is by adding control that introduces a shift in rhythmic patterns at semi periodic intervals. \\\\
There are a variety of ways to achieve control in a music generator. For rule based systems, control is added explicitly, in statistical systems (including deep learning) control is typically applied through architectural constraints and choice of training data. Certain deep learning architectures lend themselves to certain types of control. I.e an auto-regressive transformer models a sequence based on prior parts of the sequence, it natively generates continuations of an input. In deep learning, other musical parameters (i.e chord progression or melody) can be controlled for  by joint conditioning of a model, which means that the controlled musical parameters are made explicit while training. This method of adding control is quite cost-intensive requiring as much, or even more training material as the baseline model without controls. \\\\
A more economical method of adding control to a deep learning generative model is through fine-tuning and post-hoc conditioning. In fine-tuning (sometimes referred to as transfer learning) a pre-trained model is trained on additional data. There are various methods of fine-tuning, involving different configurations of the model, the model parameter and  the integration of control input, but typically fine-tuning uses considerably fewer resources and data than training a model from scratch. In post-hoc conditioning a generative model is adjusted while it is running to increase the likelihood of the output to fit the desired constraints. 
The goal of my thesis is to generate a large amount of music to extend the LastMinuteGig video game for MACT in patients with Parkinson's. LastMinutGig is a mobile application built with Unity3D, in its current iteration it contains a single button with which a player taps along to the music, controlling a set of guitar chords. When a musical change occurs the player is instructed to change their tapping. For this I will first explore and evaluate different methods of adding control to a deep learning model on a small generative music model. I will apply the most successful techniques of adding control specifically of rhythmic patterns learned from this first step to a larger generative music model, MusicLang to steer the output towards something usable in the MACT context. 
