%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Introduction ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{chap:intro}
\pagestyle{fancy}

Music is a cornerstone of modern video games. Music evokes emotional responses, triggers memories, can direct attention, and improves immersion and the overall experience of a game. Yet, it is not uncommon for players to become disengaged from the game music, often due to excessive repetition or simply different personal preferences \cite{Rogers_Weber_2019}. This can be a problem in serious games for therapeutic use, such as Last Minute Gig, a gamified Musical Attention Control Training (MACT) application aimed to improve attention control in patients with Parkinson's-disease \cite{Chalkiadakis_2022}. Ideally, MACT is a personalized experience with dynamic adjustments considering a patient's abilities and preferences. In the context of gamified self-directed therapy, this is crucial to keep patients engaged and progressing.\footnote{This type of therapy is not aimed at replacing traditional guided therapy. Ideally, it is used as a supplement,  providing relief in situations where regular treatment is not immediately available} To accommodate a wide array of patient's abilities and preferences in a MACT game, a considerable amount of musical material, specifically music that fits the constraints of MACT, is required. One potential avenue to provide this is through controlled music generation.

Over the last few years, music generation has grown from a primarily academic endeavor to a billion-dollar industry. Large technology companies are exploring music generation with foundation models such as Meta’s MusicGen \cite{copet2023simple} or Google’s MusicLM 
\cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023}. The commercial startup music generator Suno is valued at 500 million dollars. A German producer used Udio to generate a chart-topping song \cite{Ferdinand_Meyen_2024}\cite{Stassen_2024}. This breakthrough is powered by advances in language modeling, bringing about Large Language Models such as GPT-4 or LLAMA3 applied to music.

In games, generative music is promising to cater to different user preferences and enable a richer, more varied musical experience than conventional approaches to adaptive audio. Video games can include hundreds of hours of gameplay and branching storylines. Adapting the music to individual preferences adds additional complexity. It quickly is no longer feasible to manually compose unique music to fit all scenarios. Especially in more resource-constrained applications such as games for therapeutic use, generated music can offer an avenue to provide a large variety of music that can improve user engagement and the effect of the intervention.
One of the key components of a successful generative model is control. In music generation, control could be a text description or information on genre, style, and instrumentation. Control can also include time-varying, composable musical parameters such as a melody, chord progression, or musical structure. Generative models are often controlled with human-composed music. They can continue provided music, interpolate between, or inpaint - including generating accompaniments, melodies, or additional instrumental lines. In LastMinuteGig \cite{Chalkiadakis_2022} the music periodically provides stimuli, a noticeable change in the music, based on which the user changes their playing, specifically their rhythm. One way to achieve this stimulus in generated music is by adding a control that introduces a shift in rhythmic patterns at semiregular intervals. 

There are a variety of ways to achieve control in a music generator. For rule-based systems, control is integrated explicitly. In statistical systems (including deep learning) control is typically applied through architectural constraints and choice of training data. Certain deep learning architectures lend themselves to certain types of control. E.g an autoregressive transformer models a sequence based on prior parts of the sequence, it natively generates continuations of an input. In deep learning, other musical parameters (e.g. chord progression or melody) can be controlled by joint conditioning of a model, which means that the controlled musical parameters are made explicit while training. This method of adding control is quite cost-intensive, requiring as much, or even more resources as the equivalent model without controls. 

A more economical method of adding control to a deep learning generative model is fine-tuning and post-hoc guidance. In fine-tuning (also referred to as transfer learning) a pre-trained model is trained on additional data. There are various methods of fine-tuning, involving different configurations of the model, the model parameter, and the integration of control input. Typically, fine-tuning uses considerably fewer resources and data than training a model from scratch. In post-hoc guidance, a generative model is adjusted while running to increase the likelihood of the output to fit the desired constraints. The goal of this thesis is to steer a pre-trained generative model to generate music fit to a MACT context, without training a large model from scratch but using efficient fine-tuning to introduce new control mechanisms to a pre-trained model. We propse RhythmLang, a fine-tuned variant of MusicLang with controls added to steer the music towards something usable in the context of MACT. For this, We will first explore and evaluate different methods of adding control using a small generative music model. In a final step, we will evaluate the interactive potential and the potential benefits in enjoyment and player engagement of using AI-generated music over more simple generation processes in a game such as Last Minute Gig \cite{Chalkiadakis_2022}.

