%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Introduction ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{chap:intro}
\pagestyle{fancy}

Music is a cornerstone of modern video games. Music evokes emotional responses, triggers memories, can direct attention, and improves immersion and the overall experience of a game. However, \cite{Rogers_Weber_2019} finds that most players turn off game music eventually, either because of differing personal preferences, or because they find the in-game music too repetitive. This can be a problem in serious games for therapeutic use, where repeated gaming sessions are required for the intervention to be effective. In specific types of therapy such as Musical Attention Control Training (MACT), a type of therapy shown to help people with Parkinson's \cite{Park_Kim_2021} or ADHD \cite{Martin-Moratinos_Bella-Fernández_Blasco-Fontecilla_2023}, music is a crucial part of the intervention. Ideally, MACT is a personalized experience with dynamic adjustments to the music that incooperate the patient's abilities and preferences. In the context of gamified self-directed therapy, this is crucial to keep patients engaged and progressing. \footnote{This type of therapy is not aimed at replacing traditional guided therapy. Ideally, it is used as a supplement, providing relief in situations where regular treatment is not immediately available} One avenue to achieve dynamic adjustment in music, is through controlled music generation. 

Over the last few years, music generation has grown from a primarily academic endeavor to a billion-dollar industry. Large technology companies are exploring music generation with foundation models such as Meta’s MusicGen \cite{copet2023simple} or Google’s MusicLM 
\cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023}. The commercial startup music generator Suno is valued at 500 million dollars. In August 2024, a German producer used Udio to generate a chart-topping song \cite{Ferdinand_Meyen_2024}\cite{Stassen_2024}. This breakthrough is powered by advances in language modeling, bringing about Large Language Models such as GPT-4 or LLAMA3 applied to music.

In games, generative music is promising to cater to different user preferences and enable a richer, more varied musical experience than conventional approaches to adaptive audio. Video games can include hundreds of hours of gameplay and branching storylines. Adapting the music to individual preferences adds additional complexity. It quickly is no longer feasible to manually compose unique music to fit all scenarios. Especially in more resource-constrained applications such as games for therapeutic use, generated music can offer an avenue to provide a large variety of music that can improve user engagement and the effect of the intervention.
One of the key components of a successful generative model is control. In music generation, control could be a text description or information on genre, style, and instrumentation. Control can also include time-varying, composable musical parameters such as a melody, chord progression, or musical structure. Additionally, generative models are often controlled with human-composed music. They can continue provided music, interpolate between, or inpaint - which means generating accompaniments, melodies, or additional instrumental lines to an existing piece of music. 

There are a variety of ways to achieve control in a music generator. For rule-based systems, control is integrated explicitly. In statistical systems (including deep learning) control is typically applied through architectural constraints and choice of training data. Certain deep learning architectures lend themselves to certain types of control. To illustrate: An autoregressive transformer models a sequence based on prior parts of the sequence, it natively generates continuations of an input. In deep learning, other musical parameters (e.g. chord progression or melody) can be controlled by joint conditioning of a model, which means that the controlled musical parameters are made explicit while training. This method of adding control is quite cost-intensive, requiring as much, if not more resources as training an equivalent model without controls. 

A more economical method of adding control to a deep learning generative model is fine-tuning and post-hoc guidance. In fine-tuning (also referred to as transfer learning), a pre-trained model is trained on additional data, with additional control conditions carefully integrated into the model. Typically, fine-tuning uses considerably fewer resources and data than training a model from scratch. In post-hoc guidance, the intermediate outputs of a generative model are classified and adjusted during inference to increase the likelihood of the final output to fit the desired constraints. 
 
We consider Last Minute Gig \cite{Chalkiadakis_2022}, a gamified Musical Attention Control Training (MACT) application aimed to improve attention control in patients with Parkinson's-disease. In Last Minute Gig, a player taps along to the music, and is encouraged to change their tapping when the music changes. The music periodically provides stimuli, a noticeable change in the music, based on which the user changes their playing. The game uses a simple rule based system that introduces changes random changes every eight bars. We propose RhythmLang, a fine-tuned variant of MusicLang to generate Music that can provide such stimuli at controllable intervals. Specifically we are investigating bar-level control that triggers shifts in rhythmic structure, since unlike other controllable parameters such as chords or instrumentation, rhythmic structure is linked to the perceived difficulty of following the music \cite{Volk2008Syncopation}. In this way RhythmLang could generate not only a richer variety of music, but also music that is progressively more challenging for the player.   

The objective of this thesis is to develop and evaluate RhythmLang, a model that can generate complete multi-instrument musical pieces, with time-varying control over the difficulty of the piece. In our conception, rhythmic structure, is a proxy for perceived difficulty in the context of following the music in MACT. We achieve this by fine-tuning a pretrained music-generator and  introducing rhythmic controls to the training process. We will first explore and evaluate different methods of adding control using a small generative music model, and progress to larger models and more complex rhythmic controls. In a final step, we will evaluate the interactive potential, perceived difficulty and the trade-offs in enjoyment and player engagement of using music generated by RhythmLang over the simple rule-based music generation process in Last Minute Gig \cite{Chalkiadakis_2022}.

