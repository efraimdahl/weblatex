%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Literature ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Literature} \label{chap:data}
\pagestyle{fancy}

The following section contains an extensive (though not exhaustive) categorical literature study. The first part provides contextual information, including motivations, and ethical concerns as well as a comprehensive overview of techniques used in music generation. The second half is more technical, with a discussion on music representation, tokenization, control, and fine-tuning. 
As a starting point, I used the most recent ISMIR (International Society for Music Information Retrieval) papers on music generation and their referenced sources, alongside other papers suggested by my advisor, Anja Volk. In addition, I performed systematic searches using the following keywords: Deep Learning Music Generation, Diffusion Music Generation, Transformer Music Generation, Symbolic Diffusion Music Generation, and Controlled Music Generation.

\section{Why Generate Music? - Motivation}
\label{section:motivation}
\subsection{Composition Co-Pilots}
The earliest music generators (including the 18th-century musical dice games\cite{Nierhaus_2009}) were justified as methods to inspire composers and music makers, including enabling novice composers to write music. Composer David Cope\cite{Cope_1989} states that he turned to music generation to overcome writer’s block. Commercial enterprises like Suno claim to be “building a future where everybody can make great music” \cite{Suno_AI}. Many current and past efforts highlight music generation as a process that assists composers. In DeepBach\cite{Hadjeres_Pachet_Nielsen_2017}, the authors go to great lengths to make the system flexible and usable in real-life composition scenarios. Initially, they developed a MuseScore integration and later tied DeepBach into the web app NONOTO\footnote{https://github.com/SonyCSLParis/music-inpainting-ts} to support musical inpainting in Ableton Live scores. Similarly, with Composer’s Assistant\cite{Malandro_2023}, the authors explicitly enable musical inpainting and continuation in the REAPER music production software. AI has also been explored as a co-improviser for live music-making, such as in Pachet's Continuator\cite{Pachet_2003} and Ben-Tal's musical dialog system\cite{Kite-Powell_2023}.

\subsection{Music in Games}
Beyond co-creating music, AI-generated music serves functional purposes, such as background music in videos and therapy-assisting games. In gaming, AI-generated or AI-assisted music is particularly relevant due to the scale and interactivity of games.

Video games often facilitate hundreds of hours of gameplay, featuring branching storylines and complex player interactions. However, game soundtracks typically cover only a fraction of that time\cite{Plut_Pasquier_2020, Worrall_Collins_2024}. Through different adaptive techniques, relatively short snippets of original material can be stretched into hours of unique audio, often relying on the recombination of different elements. However, rule-based recombination has its limitations. A recent study of player behavior \cite{Rogers_Weber_2019} finds that many players eventually turn off game music. They cite various reasons, such as preferring their own music over the game soundtrack (46.7\%) or finding the in-game music repetitive (29.6\%).

Procedural generation of 3D assets, levels, and enemy behavior is commonplace, but music generation remains underutilized. Composing and adapting music for every possible scenario would be tedious. AI-assisted music composition could enable adaptive audio on a large scale, either by creating numerous musical assets anticipating player choices or generating new variations of the game score in real time to enhance player immersion. However, several challenges make AI music generation difficult in games. Performance issues arise due to the resource-intensive nature of AI generators. Additionally, AI-generated music is difficult to control, and there is little guarantee that the generated tracks will be appropriate\cite{Plut_Pasquier_2020}. Furthermore, AI music generators currently lack proper integration into video game environments and engines\cite{Worrall_Collins_2024}.

\subsection{Music in Serious Games}
Aside from games for general audiences, generative music has potential applications in serious games. Serious games are designed for purposes beyond entertainment, such as education or therapeutic use, including music therapy\cite{Djaouti2011}. In music therapy, music can be used for emotion regulation, motivation, adherence, motor coordination, rhythmic entrainment, and facilitating social interactions\cite{musicwellbeing_agres_2021}. Musical attention control training has been shown to help individuals with Parkinson’s\cite{Park_Kim_2021}, ADHD\cite{Martin-Moratinos_Bella-Fernández_Blasco-Fontecilla_2023}, autism\cite{Pasiali_LaGasse_Penn_2014}, and psychosis\cite{van_Alphen_Stams_Hakvoort_2019} improve their mental capabilities for selective and switching attention. Serious games have the potential to supplement music therapy. “Last Minute Gig”\cite{Chalkiadakis_2022} implements clinical music therapy protocols as a serious game to improve attention control in Parkinson’s patients. However, users reported boredom and a lack of feedback, while more musically experienced users felt less challenged. Schlette\cite{Schlette_2022} attempted to address these issues by introducing dynamic difficulty adjustment through a feedback system alongside more complex music generation. This thesis aims to develop a controlled music generation model to improve player engagement through a richer music system.
\section{Why Not Generate Music? - Ethical Concerns} \label{section:ethical}
\subsection{Introduction}
There are several concerns related to AI-based music generation. First, there are legal concerns regarding copyright and licensing. Generative models may produce outputs that are identical or highly similar to copyrighted training data. Furthermore, the question remains whether models should be allowed to train on copyrighted data in the first place. Broader concerns include the devaluation of human labor and creativity, the oversaturation of cultural spaces with low-quality generated content, and the environmental impact of large generative models, which require substantial energy, water, and rare materials.

\subsection{Data Leakage and Copyright}
Generative AI companies are achieving record-breaking valuations, including music generators, with Suno at a valuation of about 500 million dollars after a 125  million dollar fund-raiser leading the pack. \cite{Stassen_2024} \cite{Tencer_2024}. However, AI companies are facing backlash from artists and record labels, with an organization of record labels including the so-called big three Sony, Warner Music, and UMG suing Suno and Udio for \$150.000 dollars per infringed work\cite{Kaba_River_Perry_2024}.  Generative AI runs a substantial risk of parroting or leaking training data. In language models, the leakage problem is of concern when training on data that contains sensitive information, which may be revealed either through accidental leakage or through membership inference attacks \cite{Duan_Suri_Mireshghallah_Min_Shi_Zettlemoyer_Tsvetkov_Choi_Evans_Hajishirzi_2024}. While leakage may raise privacy concerns in other generative models such as speech and image-generators \cite{Carlini_Hayes_Nasr_Jagielski_Sehwag_Tramèr_Balle_Ippolito_Wallace_2023} for music, the risk of training data leakage is mostly an issue of copyright. Ed Newton Rex shows some examples of how Suno can be influenced \cite{Newton-Rex_2024} to leak training data. This ability to create disconcertingly close reproduction of copyrighted work is also cited in court documents. Suno has since started to prevent prompting with artist names (i.e. in the style of Eminem) and including known song lyrics. 

\subsection{Training and Copyright}
Besides leaking training data, there is the more general question of whether AI models should be allowed to train on unlicensed work. Echoing the court case between OpenAI and the New York Times \cite{Reed_2024}, both Suno and Udio cite fair use in response to accusations of copyright infringement. In US copyright law the fair-use clause limits exclusive rights to a work, with four factors to consider: 1) purpose and character of the work in use, 2) nature of copyrighted work, 3) amount of the copyrighted work used, and 4) the effect on the potential market or value of copyrighted work.\cite{copyrightlaw}   Fair use is often granted to derivative works such as parodies and covers and works used in educational settings. AI’s learning of musical structures and patterns has also been compared to the human learning process: Humans learn based on copyrighted music they listen to without giving credit or compensation to their influences. Newton-Rex \cite{Newton-Rex_2024} rejects this comparison. In learning music, humans contribute to the musical ecosystem by taking lessons, going to performances, or at the very least generating some streaming revenue for artists, none of these are true for machine learning models learning from scraped data. 

\subsection{Devaluing Music}
While few follow director Ram Gopal Varma’s viral announcement to only use AI-generated music in his future films \cite{Singh_2024}, AI-generated music is becoming increasingly difficult to differentiate from human production and already receiving considerable amounts of streams. It is not unlikely that music in film, video, and game projects may be supplemented with, if not replaced by AI-generated music. The online music market is saturated, with more than 100,000 songs uploaded to music streaming giant Spotify every day \cite{Stassen_2023}. Generative AI will likely exacerbate this problem, resulting in a race to the bottom for creatives and musicians. 

\subsection{Environmental Impact}
Digital infrastructure, traditional data centers, crypto-currency mining, and AI-centered data infrastructure account for about 2\% of the world's energy consumption \cite{Marechal_2024}. The performance of current large language models often scales with simultaneous increases in model size, training data, and computation time.\cite{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020}  Each of these three factors requires considerable resources. Music generation is no exception. In a recent ISMIR publication \cite{Holzapfel_Kaila_Jääskeläinen_2024} the authors make estimates on energy consumption of different projects related to music generation and computation-intensive MIR, finding an average energy consumption of 224.8kWh for model training (the energy consumption of an average western person over 2 months). The energy consumption is divided highly unevenly, with the median being at merely 18 kwh (3 days of an average westerner's energy consumption). Music generation models associated with large technology companies are responsible for about 89\% of the estimated energy use. This is only for training, models that are deployed publicly, continue to use substantial energy for inference. Beyond just the carbon footprint of generative AI, the local impacts of resource use, such as rare minerals and water, are crucial to examine. 

\section{Overview of music generation} \label{section:music_generation_overview}
From antique wind-chimes to classical period musical dice games to the aleatoric music of the 20th century - humans have used algorithmic, probabilistic, and statistical methods to create music. As early as the late 1940s, computers have played a role in composition as sound generators, instruments \cite{France-Presse_2016} and providing musical material themselves, such as in the 1957 Illiac suite \cite{Hiller_Isaacson_1959}. The following 50 years are characterized by disparate, academic experiments in music generation, primarily in the symbolic music domain. They utilize a variety of contemporary AI technologies, from expert systems and ontologies \cite{Hiller_Isaacson_1959}\cite{Ebcioğlu_1994} to evolutionary algorithms \cite{Polito_Daida_Bersano-Begey_1997} to feed-forward \cite{Todd_1989}, recursive \cite{Mozer_1994}, and convolutional neural networks \cite{coconet}. Some composers in the classical tradition, such as Iannis Xenakis \cite{Xenakis_1992} and David Cope\cite{Cope_1989} use computer algorithms in their creative work. 
In the 2010s a small ecosystem of commercial generative music startups such as Jukedeck, PopGun, and Ampermusic \cite{Featherstone_2017} starts to emerge, alongside an increasing number of publications applying deep learning, particularly Generative Adversarial Networks (GANs), and Recursive Neural Networks (RNN) to music including MIDInet \cite{midinet}, DeepBach \cite{Hadjeres_Pachet_Nielsen_2017} and FolkRNN \cite{Sturm_Ben-Tal_2016}. With the development of the transformer architecture in 2017 \cite{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017}, large technology companies start experimenting with music generators including OpenAI’s Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} and Musenet\cite{Christine_2019}, Meta’s MusicGen\cite{copet2023simple} and Google’s MusicLM \cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023} and the preceding Magenta project, with fully generative models capable of producing sequences of high-quality music modeled from raw audio. At the time of writing, commercial music generators such as Suno and Udio are raising millions of dollars in investment funds\cite{Stassen_2024}, while generated music is being widely streamed and actively used in TV and video productions. 

\section{Non-neural music generation} \label{section:non_neural_generation}

\subsection{Why look beyond deep learning}
Neural, specifically deep learning (DL) systems, currently receive considerable attention. However, given their substantial drawbacks relating to explainability, transparency, computational efficiency, copyright and licensing issues, and their enormous need for data (more in section \ref{section:ethical}), it is worthwhile considering alternative approaches to music generation. A recent study \cite{Yin_Reuben_Stepney_Collins_2023} performs a comprehensive listening survey comparing neural net and non-neural net systems. The top-performing systems a Markov Model -  MAIA Markov \cite{Collins_Laney_2017}  and a deep learning system  - MusicTransformer \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018} perform similarly well in the listening study. The choice of one of the earliest transformer-based music from 2018 \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018} and the restriction to symbolic music, raises questions, whether their conclusion of similar performance still holds. Considering the study was published in 2023, these are important limitations. However, their criticism that many DL-based music generation projects do not look beyond DL and compare their systems based on technical metrics - with no obvious impact on how human listeners perceive the output remains solid. Finally, many hybrid approaches successfully combine traditional rule-based or statistical methods with deep learning. Those methods may help researchers and developers maintain a more comprehensive toolkit of techniques and paradigms. 

\subsection{Rule-based music generation}
Non-neural net systems can classified as either rule-based or statistical. Both types have been part of some of the earliest attempts at music generation. Centuries of style-defining musicological writing from ancient Mesopotamian tuning charts \cite{Mirelman_2013} to Fux`s Gradus ad Parnassum \cite{Fux_1725} and Arnold Schönberg`s 12-tone music have crystallized sets of rules that approximate various styles of music. Many approaches to music generation take advantage of this knowledge and codify it into a computer program, creating expert systems for music generation. In Hiller \& Isaacson`s Illiac Suite, a series of experimental, computational compositions, the first and second movements are generated following the rules of first species counterpoint \cite{Fux_1725}, approximating Palestrina's contrapuntal technique. Some rules aim to contain the melody, such as limiting the range to an octave, enforcing the identical start and end notes, and avoiding consecutive melodic jumps. Other rules aim to constrain harmony, such as forbidding parallel octave, fifth, and fourth motion and enforcing consonant harmonies. Hiller and Issacson's approach is relatively simple, using only a handful of conditions (see figure \ref{fig:hillerissacson}). Rule-based generation can be highly complex, such as CHORAL \cite{Ebcioğlu_1994} (for which the developer also built a custom programming language), which encodes over 300 rules to realize bach-style chorales from a given melody. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{IMAGES/IlliacRuleBased.jpg} 
    \caption{Rule-Based - Block Diagram from Hiller and Issacson’s book  - explaining movement two of the 1957 Illiac Suite. }
    \label{fig:hillerissacson}
\end{figure}

\subsection{Markov Model Music Generation}
Markov models remain a popular method of generating music to this day. At its simplest, a Markov music generator could work off of a transition matrix for pitch classes, such as Richard Pinkerton’s 1956 “Banal Melody Generator” \cite{Pinkerton_1956}. 
Markov chains can be nested or constrained for more complex interactions \cite{Collins_Laney_2017}. In Hiller and Isaacson's \cite{Hiller_Isaacson_1959} fourth movement of the Illiac suite, they use Markov chains with a table of possible intervals stretching from unison to octave. In the latter sections of the movement, they introduce additional restrictions to add memory to the method through higher-order Markov chains that reference previously generated music. 
Transition matrices can be built from very little data, such as short improvisations \cite{Pachet_2003}, but training over a whole corpus is also viable. Other systems configure Markov chains to take additional inputs into account, such as Allan, \cite{Allan_2002} who generates harmonies to given melodies in the style of Bach. This makes Markov model-based systems very flexible and relatively lightweight.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{IMAGES/PinkertonNurseryRhymes.jpg} 
    \caption{Transition matrix from Pinkerton's 1956 “Banal Music Generator”. Probability of a pitch (row) following on a pitch (column), likely pairs are marked in yellow. The probabilities are calculated from a set of 39 nursery rhymes.}
    \label{fig:pinkertonmatrix}
\end{figure}\subsection{Other statistical approaches}
Music generation has also been attempted with other means, such as metaheuristic search for harmony or melody \cite{Altay_Alatas_2018}. In Morpheus, \cite{Herremans_Chew_Morpheus_2019} the developers use variable neighborhood search (VNS) to generate polyphonic pieces following a tension profile for long-term structure. Closely related to this approach are evolutionary and genetic algorithms such as Politio et al's \cite{Polito_Daida_Bersano-Begey_1997} model of 16th-century counterpoint as a multi-population problem. Here, three separate populations of agents generate instructions focusing on different musical aspects, such as harmony or imitation, and are evaluated based on individual performance and symbiotic performance together with the other agents over multiple generations. 


\subsection{Early Neural Net based systems}

Music generators based on neural nets were introduced as early as 1989. Todd et al. \cite{Todd_1989} generate melodies using a fixed window for a conventional feed-forward neural network but also introduce a feedback loop feeding the network's previous state to the next iteration. Future work based on RNNs uses the latter principle, such as CONCERT,\cite{Mozer_1994} a 1994 RNN trained to generate melodies based on datasets of Bach chorales, waltzes, and European folk songs. There are also hybrid systems such as HARMONET \cite{Hild_Feulner_Menzel_1991}, an RNN-based music generator for re-harmonizing Bach chorales. It merges the RNN with a symbolic rule-checking algorithm. More recent RNN-based music generators, such as FolkRNN \cite{Sturm_Ben-Tal_2016}, a melody generator trained on Irish folk songs, use long short-term memory (LSTMs) or gated recurrent units (GRUs). Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) remain popular architectures for music generation. \cite{Civit_Civit-Masot_Cuadrado_Escalona_2022}

\section{Deep learning for music generation}\label{section:deep_learning_generation}
State-of-the-art music generation, including many commercial applications, leverages the advances in language and image generation over the last five years. Two distinct approaches, namely autoregressive, transformer-based, and diffusion-based approaches dominate.


\subsection{Transformers - sequence modeling without recurrence}
Our ability to model complex sequences, including music, improved dramatically with the development of the transformer model.  Transformers were originally developed for the task of language translation.\cite{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017}. A sequence is modeled as a string of \textit{tokens}, a numeric representation of parts of the sequence. In natural language, a token could be a numeric representation of a word or a character. Transformers deploy the self-attention mechanism already deployed in LSTMs for sequence modeling tasks \cite{Sutskever_Vinyals_Le_2014} but replace the recurrent connection with positional embeddings and masked attention. This allows the model to train on all tokens in parallel instead of one token at a time, which enables much larger and more capable models trained at a fraction of the time required to train similarly large RNNs. The transformer comes in several different configurations. The original transformer contains encoder and decoder layers - see figure \ref{fig:transformer}. Tasks relating to sequence understanding, such as music classification, often use an encoder-only architecture. The BERT series of language models \cite{Devlin_Chang_Lee_ToutanovaBERT_2019} and music understanding models such as MusicBERT\cite{Zeng_Tan_Wang_MUSICBERT_2021} are examples. 
On the other hand, sequence generation models often employ a decoder-only architecture, including the GPT-series \cite{Radford_Wu_Child_Luan_gpt2_2019} and many music generators such as MusicGen\cite{copet2023simple}. The transformer architecture is the baseline for all current large language models. Transformers find use in modalities beyond text, including images, audio, or DNA sequences. 

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{IMAGES/Transformer,_full_architecture.png} 
\caption{Schema of the full transformer encoder-decoder architecture}
\small\textsuperscript{a=} By dvgodoy - CC BY 4.0, https://commons.wikimedia.org/w/index.php?curid=151216016
\label{fig:transformer}
\end{figure}


\subsection{Diffusion models - spectrograms and piano-rolls}
Diffusion models are widely used for image and audio generation. Diffusion models learn to remove noise from a distribution (i.e. an image). Random noise is added to an image, and the model learns to undo this addition. During inference, the model starts with random noise (often accompanied by a guiding text prompt) and undoes it until it arrives at a clear image. AudioLDM \cite{Liu_Chen_Yuan_Mei_Liu_Mandic_Wang_Plumbley_2023}, and StableAudio \cite{Evans_Parker_Carr_Zukowski_Taylor_Pons_2024} are diffusion models that operate based on continuous audio encoding by a variational autoencoder. Diffusion models have also been used to generate symbolic music. Polyffusion \cite{Min_Jiang_Xia_Zhao_polyffusion_2023} uses image representations of piano rolls and adapts their diffusion model for various tasks, inpainting, accompaniment, and melody generation and generation based on a given chord sequence or texture. 

\subsection{Token Sampling} \label{section:token_sampling}
Transformer models are next token predictors, essentially large classification models that choose the next probable token given the prior sequence. The methods of choosing the next token in a transformer vary. A pool of tokens from which the model chooses is called the \textit{vocabulary} of the model. The size of this vocabulary is determined by the tokenization method (see section \ref{section:tokenization}) Typically the last layer is a so-called softmax layer, which transforms the output vector into an equally sized vector of probabilities adding up to one. \cite{Radford_Wu_Child_Luan_gpt2_2019}. This is calculated as follows. 
\begin{equation}
\sigma(z_i) = \frac{\exp(z_i / T)}{\sum_{j} \exp(z_j / T)}
\end{equation}
This last layer then forms the basis of different sampling methods. The most simple method is greedy sampling, simply choosing the most probable token. More typical sampling methods are probabilistic, where the next token is sampled from the vector of probabilities. The temperature parameter $T$ in the above equation, can often be controlled by the user to shape the probability distribution. At a high temperature, the probabilities are distributed more uniformly across the vector, the output becomes more noisy. At a low temperature, only the highest probability tokens remain, making the output more deterministic. 
A common variant is top k sampling, where only the most probable k tokens are considered. In top p sampling, the most probable tokens are added up until hitting a target percentage p. 

\input{BODY/2_technicallit}
