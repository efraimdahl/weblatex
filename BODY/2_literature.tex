%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Literature ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Literature}
\label{chap:data}
\pagestyle{fancy}

The following section contains an extensive (though not exhaustive) categorical literature study. The first part provides contextual information, including an overview of the history of music generation, motivations and ethical concerns. The second half is more technical with a discussion on basic concepts such as music representation and recurring challenges in music generation such as control. This is followed by a high-level discussion on the techniques used in music generation in the past. Finally, I will be discussing in more detail the models most used in recent research. In particular I will review different methods of adding control for these models.  As a starting point I used the most recent ISMIR (International Society for Music Information Retrieval) papers on music generation and their referred sources alongside other papers suggested by my advisor Anja Volk. In addition I performed systematic searches with the following keywords. Deep Learning Music Generation, Diffusion Music Generation, Transformer Music Generation, Symbolic Diffusion Music Generation, Control Music Generation. 

\section{Brief History of Music Generation}
From antique wind-chimes to classical period musical dice games, the aleatoric music of the 20th century, all the way to deep learning based music generators -humans have used algorithmic, probabilistic, and statistical methods to create music. As early as the late 1940s computers have played a role in composition as sound generators and instruments \cite{France-Presse_2016} and providing musical material themselves such as in the 1957 Illiac suite \cite{Hiller_Isaacson_1959}. Over the next 50 years, there are a lot of mostly academic, quite disparate experiments in music generation most of them in the symbolic music domain. They utilize a variety of contemporary AI technologies, from expert systems and ontologies to evolutionary algorithms to feed-forward, recursive, and convolutional neural networks. Some composers in the classical tradition such as Iannis Xenakis \cite{Xenakis_1992} and David Cope \cite{Cope_1989} use computer algorithms in their creative work. 
In the 2010s a small ecosystem of commercial generative music startups such as Jukedeck, PopGun, and Ampermusic \cite{Featherstone_2017} started to emerge, alongside an increasing number of publications applying deep learning, particularly GANs, and RNNs to music including MIDInet \cite{midinet}, \cite{Hadjeres_Pachet_Nielsen_2017} and Sturm and Ben-Tal’s work on FolkRNN starting in 2016 \cite{Sturm_Ben-Tal_2016}. With the development of the transformer architecture in 2017 (Vaswani et al., 2017), large technology companies start experimenting with music generators including OpenAI’s Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} and Musenet\cite{Christine_2019}, Meta’s MusicGen\cite{copet2023simple} and Google’s MusicLM \cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023} and the preceding Magenta project, with fully generative models capable of producing sequences of high-quality music modeled from raw audio. At the time of writing, commercial music generators such as Suno and Udio are raising millions of investment funds\cite{Stassen_2024}, while generated music is being widely streamed and actively used in TV and video productions. 

\section{Why generate music? - Motivation}
\subsection{Composition co-pilots}
The earliest music generators (including the 18th century musical dice games), where justified as methods to deliver inspiration to composers and music makers, including enabling novice composers to write music. Composer David Cope\cite{Cope_1989}, states that he turned to music generation to overcome writer’s block. Commercial enterprises like Suno, claim to be “building a future where everybody can make great music” \cite{Suno_AI} Many current and past efforts point to music generation as a process that accompanies composers. In DeepBach\cite{Hadjeres_Pachet_Nielsen_2017}, the authors go through considerable difficulties in building the system to be very flexible and usable in real-life composition systems. Initially, they developed a muse-score integration and later tied DeepBach into the web app NONOTO\footnote{ https://github.com/SonyCSLParis/music-inpainting-ts} to support musical inpainting in Ableton Live scores. Similarly with Composer’s Assistant\cite{Malandro_2023} the authors explicitly enable musical inpainting and continuation in the REAPER music production software. These efforts to make music generators accessible to a wider audience of music makers by integrating them into common software are commendable and are an important step for musical co-creation.

\subsection{Music in games}
Beyond the co-creation of music, there is a range of music that serves a functional rather than a purely aesthetic purpose. This can be background music in videos and therapy assisisting games. In games, one can make the case for AI-generated music or AI-assisted music due to their size and interactability. 
Video games can facilitate often hundreds of hours of gameplay, they can have branching storylines and complex player interactions. Game sound tracks typically dont cover more than a fraction of that time.\cite{Plut_Pasquier_2020}\cite{Worrall_Collins_2024}. 
Through different adaptive techniques, relatively short snippets of the original material can be stretched into hours of unique audio often relying on the recombination of different elements, but rule-based recombination only goes so far. A recent study of player behavior \cite{Rogers_Weber_2019} finds that many players turn off the game music eventually. They cite different reasons, i.e. preference for their own music over the game music (46.7\%) or the repetitiveness of in-game music (29.6\%). 

The procedural generation of 3D assets and levels and enemy behavior is quite commonplace. This is rarely applied to music. Composing and adapting music to fit every scenario possible would be tedious. With AI-assisted music composition, it could become possible to enable adaptive audio on a large scale. Either by creating a large number of musical assets anticipating player choice, or generating new variations of the game score on the fly, improving player immersion through real-time adaptation. Several challenges make music generation difficult in games. There are performance issues: AI generators are often resource intensive. Additionally they are difficult to control, and there is little guarantee that the generated tracks will be appropriate. \cite{Plut_Pasquier_2020} Currently, they also lack proper integration into video game environments and engines. \cite{Worrall_Collins_2024}.

\subsection{Music in serious game}							
Aside from games for general audiences, there is a potential use for generative music within serious games in the context of music therapy. In music therapy, music can be used for emotion regulation, motivation, adherence, motor coordination, (rhythmic) entrainment, and to facilitate social interactions. \cite{musicwellbeing_agres_2021} Musical attention control training for instance has been shown effective in helping patients with Parkinson’s\cite{Park_Kim_2021}, ADHD \cite{Martin-Moratinos_Bella-Fernández_Blasco-Fontecilla_2023}, autism \cite{Pasiali_LaGasse_Penn_2014} and psychosis \cite{van_Alphen_Stams_Hakvoort_2019} improve their mental capabilities for selective attention and switching attention. Serious games have the potential to supplement music therapy. “Last Minute Gig” \cite{Chalkiadakis_2022} implements clinical music therapy protocols as a serious game to improve attention control in Parkinson's patients. This thesis works towards expanding the game with generated music to improve player experience.


\section{Why not generate music - Ethical Concerns}
\subsection{Introduction}
There are several concerns related to the AI-based generation of music. First, there are basic legal concerns about copyright and licensing. During the generation process generative models may produce output that is identical or very similar to (copyrighted) training data. More fundamentally, during training the question remains whether models should be allowed to train on copyrighted data in the first place. More broadly, and this also applies to non-deep learning-based generators, are concerns about the devaluation of human labor and creativity, the flooding of our cultural spaces with low-quality generated content. Finally, there are concerns about the environmental impact, especially of large generative models, which require substantial amounts of energy, water, and rare materials to run. 

\subsection{Data Leakage and Copyright}
Generative AI companies are achieving record-breaking valuations and this includes music generators with Suno at a valuation of about 500 million dollars after a 125  million dollar fund-raiser leading the pack. \cite{Stassen_2024} \cite{Tencer_2024}. However, AI companies are facing backlash from artists and record labels with an organization of record labels including the  “big three” Sony, Warner Music, and UMG suing Suno and Udio for \$150.000 dollars per infringed work\cite{Kaba_River_Perry_2024}.  Generative AI runs a substantial risk of parroting or leaking training data. In language models, the leakage problem is of concern when training on data that contains sensitive information, which may be revealed either through accidental leakage or through membership inference attacks \cite{Duan_Suri_Mireshghallah_Min_Shi_Zettlemoyer_Tsvetkov_Choi_Evans_Hajishirzi_2024}. While leakage may raise privacy concerns in other generative models such as speech and image-generators \cite{Carlini_Hayes_Nasr_Jagielski_Sehwag_Tramèr_Balle_Ippolito_Wallace_2023} for music, the risk of training data leakage is mostly an issue of copyright. Ed Newton Rex shows some examples of how Suno can be influenced \cite{Newton-Rex_2024} to leak training data. This ability to create disconcertingly close reproduction of copyrighted work is also cited in the court documents. Suno has since started to prevent prompting with artist names (i.e. in the style of Eminem) and including known song lyrics. 

\subsection{Training and Copyright}
Besides leaking training data, there is the more general question of whether AI models should be allowed to train on unlicensed work. Echoing the court case between OpenAI and the New York Times \cite{Reed_2024}, both Suno and Udio cite fair use in response to accusations of copyright infringement. In US copyright law the fair-use clause limits exclusive rights to a work, with four factors to consider: 1) purpose and character of the work in use, 2) nature of copyrighted work, 3) amount of the copyrighted work used, and 4) the effect on the potential market or value of copyrighted work.\cite{copyrightlaw}   Fair use is often granted to derivative works such as parodies and covers and works used in educational settings. AI’s learning of structures has also been likened to the human learning process, humans learn based on copyrighted music they listen to, without giving credit or compensation to their influences. Newton-Rex \cite{Newton-Rex_2024} rejects this comparison. In learning music, humans contribute to the musical ecosystem, they take lessons, go to performances, or at the very least generate some streaming revenue for artists, none of these are true for machine learning models learning from scraped data. 

\subsection{Devaluing Music}
While few are following director Ram Gopal Varma’s announcement to only use AI-generated music in his future films \cite{Singh_2024}, AI-generated music is becoming increasingly difficult to differentiate from human production and already receiving considerable amounts of streams and it is not unlikely that music in film, video and game projects may be replaced or at least supplemented with AI-generated music. The online music market is saturated, with more than 100,000 songs uploaded to music streaming giant Spotify every day \cite{Stassen_2023}. Generative AI may just further exacerbate this problem, resulting in a race to the bottom for creatives and musicians. 

\subsection{Environmental Impact}
Digital infrastructure, traditional data centers, crypto-currency mining, and AI-centered data infrastructure account for about 2\% of the world's energy consumption \cite{Marechal_2024}. The performance of current large language models often scales with simultaneous increases in model size, training data, and computation time.\cite{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020}  Each of these three factors requires considerable resources. Music generation is no exception. In a recent ISMIR publication \cite{Holzapfel_Kaila_Jääskeläinen_2024} the authors make estimates on energy consumption of different projects related to music generation and computation-intensive MIR, finding an average energy consumption of 224.8kWh for model training (the energy consumption of an average western person over 2 months). The energy consumption is divided highly unevenly, with the median being at merely 18 kwh (3 days of an average westerner's energy consumption). Music generation models associated with large technology companies are responsible for about 89\% of the estimated energy use. This is only for training, models that are deployed publicly, continue to use substantial energy for inference. Beyond just the carbon footprint of generative AI, the local impacts of resource use such as rare minerals and water are important to keep in mind. 

\subsection{How  I plan to address these concerns}
During this thesis process, I’m planning to address the outlined ethical concerns in the following manner. First, I will only train, and use open-source models trained on licensed data providing attributions where possible. I will also make my trained models publicly available with substantial documentation to contribute to the open-source ecosystem, research, and music. Second I am explicitly designing the generative process to be cooperative, this is facilitated on the one hand through the introduction of new control modalities, and on the other the choice of symbolic music over audio, which eases editing and integration into composition software. Finally, I am not attempting to train custom large foundation models, rather I’m trying to find ways to extend existing models to introduce new control mechanisms, requiring training on only a fraction of the model parameters, with substantially less need for data, computation, and energy. 

\section{Representation}
\label{representation}
\subsection{Symbolic vs Audio}
Music can be digitally represented in two ways, either directly as audio or symbolically as a digital score. Working with different representations comes with different drawbacks and benefits. Symbolic data is inherently a condensed form, it contains no or very little information on many acoustic features such as timbre and acoustic space, it is also less precise in terms of timing and pitch due to inherent quantization when transcribing. Symbolic data is also far less available than audio data, many symbolic music datasets are created by compiling hand-transcribed music. High-quality automatic transcription remains an unsolved issue.\cite{Ji_Yang_Luo_survey_symbolic_2024}\cite{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024}  However, symbolic music gives more direct access to many higher-level musical features such as chord progressions, melodies, and instrumentation. When working with audio these features first have to be extracted, which often requires working with additional deep learning models. Another consideration to take into account is size. Raw audio is significantly larger than a corresponding digital score. With techniques such as the variational autoencoder \cite{Kingma_Welling_2014}, audio can be compressed quite considerably for use in machine-learning environments, however, this introduces another costly preprocessing step. In addition, while audio is easier to listen to directly (symbolic data needs to be rendered first), it is difficult to edit once generated. 


\subsection{Tokenisation}
Sequences are typically transformed into tokens, a numerical representation of data, to be handled by a machine learning algorithm. Audio-based music generation uses tokenization also as a way to condense audio while retaining its semantic content. Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} uses a variational autoencoder\cite{Kingma_Welling_2014} with a discretizing bottleneck (VQ-VAE) to create tokens from audio. Musicgen \cite{copet2023simple} tokenizes audio using the previously developed Encodec model for audio compression, which similarly to VQ-VAE learns a highly condensed discrete representation of audio \cite{Défossez_2023_encodec}. These condensed encodings are crucial for generative modeling, also for diffusion models.

\subsection{Symbolic Tokenisation}
\label{symbolic tok}
Unlike audio, symbolic music is typically already highly condensed, and a variety of different tokenizers exist for different tasks. \cite{Fradet_Briot_Chhel_2021}. Common ways of tokenizing symbolic music align closely with the msical instrument digital interface (MIDI) standard, with individual tokens encoding different midi-events such as note-on, note-off, velocity. The REMI+ \cite{Huang_Yang_remi_pop_transformer_2020} tokenisation expands on MIDI-based tokenisation with tokens for bar and position, which is designed to help capture recurring musical patterns. The PerTok tokenizer designed by Lemonaid\footnote{https://www.lemonaide.ai/}  encodes micro timings and offsets designed to capture the full spectrum of rhythm in musical performances.
These extensions come at a cost, the resulting sequences can become very long, which adversly affects the model\cite{Ji_Yang_Luo_survey_symbolic_2024}. Attempts have been made to condense multiple tokens, such as compound word, or nested tokens.\cite{Ryu_Dong_nested_2024}. In MMT \cite{Dong_Chen_MMT_Kirkpatrick_2023}, tuples of midi-data are combined into single tokens. Our tokenisation varies by task, but in general encodes events into single characters. This is then combined with a byte pair tokenizer, which combines the most common byte-pairs (extracted in a prior run over the input data) into compound tokens until the vocabulary size is exhausted. The vocabulary size indicates the size of the embedding layer, it is orders of magnitude larger than the characters used for the single events.\cite{Sennrich_Haddow_Birch_BPE_2016}. This approach mimics the compound word approach by condensing the most common combinations into single tokens. This drastically shortens the sequence length by about a third and improves modeling. More in the appendix \ref{section:compare_sym}

See table \ref{table:bigtable} in the appendix for a more elaborate display of representations used in symbolic music generators



\input{BODY/2_technicallit}
