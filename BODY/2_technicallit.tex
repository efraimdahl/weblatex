\section{Deep Learning and Symbolic Music Generation}
There are various different approaches to symbolic music generation described that use deep learning. However there are considerable differences in the implementation such as choice of architecture and representation that influence what features can be controlled for. The following table summarizes theese key characteristics across a selection of approaches that inspire our approach. For a comprehensive survey of representation, tasks, and evaluation methods used in symbolic deep learning music generation see \cite{Ji_Yang_Luo_survey_symbolic_2024}. 

\begin{table}[H] \label{bigtable}
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height
    \setlength{\tabcolsep}{3pt} % Adjust column spacing
    \scriptsize % Set smaller font size
    \begin{tabular}{|p{2.5cm}|p{1.8cm}|p{3cm}|p{1cm}|p{1cm}|p{3cm}|p{2.5cm}|}
        \hline
        \textbf{Name} & \textbf{Architecture} & \textbf{Control} & \textbf{TV} & \textbf{MI} & \textbf{Dataset} & \textbf{Representation} \\
        \hline
        FolkRNN 2015 \cite{Sturm_Ben-Tal_2016} & RNN & meter, mode & No & No & TheSession\cite{sessionfolkdata} & REMI-Like\\
        DeepBach 2017 \cite{Hadjeres_Pachet_Nielsen_2017} & RNN & Inpainting & Yes & Yes & JSB-chorales\cite{jsbchorales} & Midi-Like\\ 
        MusicTransformer (2018) \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018} & Transformer & - & No & No & Maestro\cite{hawthorne2018maestro},  JSB-chorales\cite{jsbchorales} & Midi-Like\\
        MuseNet 2019 \cite{Christine_2019} & Transformer & instrument, genre & Yes & Yes & Mastro\cite{hawthorne2018maestro}, CX\cite{classicalarchives}, BM\cite{bitmidi} & ?\\
        MidiNet 2019 \cite{midinet} & GAN & chords, melody & Yes & Yes & HTPM\cite{hooktheorypopmidi} & Midi-Like\\
        Fader Nets 2020\cite{Tan_Herremans_2020} & VAE & arousal & No & No & Maestro \cite{hawthorne2018maestro} & Custom \\
        MultitrackMusicMachine 2020 \cite{Ens_Pasquier_2020_MMM} & Transformer & inpainting, instrument, note-density & Yes & Yes & Lakh MIDI\cite{Raffel_2016} & MMM\\
        PopTransformer 2020 \cite{Huang_Yang_remi_pop_transformer_2020} & Transformer & chords, tempo & Yes & No & Custom & REMI\\
        Museformer 2022 \cite{Yu_Lu_Wang_Hu_Tan_Ye_Zhang_museformer_2022} & Transformer & - & No & Yes & LakhMIDI \cite{Raffel_2016} & REMI-Like\\
        Polyfussion 2023 \cite{Min_Jiang_Xia_Zhao_polyffusion_2023} & Diffusion & inpainting, texture & Yes & No & POP90 \cite{Wang_Chen_pop90_dataset} & Piano-Roll\\
        FIGARO 2023 \cite{Rütte_figaro_2023} & Transformer & chords, instrument, meter, note-density & Yes & Yes & Lakh MIDI\cite{Raffel_2016} & REMI+ \\
        MMT 2023 \cite{Dong_Chen_MMT_Kirkpatrick_2023} & Transformer & instrumentation & No & Yes & Lakh MIDI\cite{Raffel_2016}, SOD \cite{Crestel_OrchestralDataset} & Midi-Tuple \\
        Sympack 2024 \cite{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024} & Transformer & chords, structure, notes & Yes & Yes & Lakh MIDI\cite{Raffel_2016}, \cite{Bertin-Mahieux_Ellis_Whitman_Lamere_2011}, Custom & -\\
        MuseCoco 2023 \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} & Transformer & various & No & No & Custom & REMI-Like\\
        MBC 2024 \cite{Shu_Xu_Musebarcontrol_2024} & Transformer & chords & Yes & No & POP90\cite{Wang_Chen_pop90_dataset} & REMI-Like\\
        NTT 2024\cite{Ryu_Dong_nested_2024} & Transformer & - & No  & Yes & Lakh MIDI\cite{Raffel_2016}, POP90\cite{Wang_Chen_pop90_dataset}, SOD\cite{Crestel_OrchestralDataset} & Compound\\
        FTG 2024\cite{Zhu_Liu_Jiang_Zheng_texture_2024} & Diffusion & texture, rhythm, chords & Yes & No & POP90 \cite{Wang_Chen_pop90_dataset} & Piano-Roll \\
        NDRD 2024\cite{Huang_rule_diffusion_2024} & Diffusion & chord, pitch, note-density & Yes & No & Maestro \cite{hawthorne2018maestro}, POP90\cite{Wang_Chen_pop90_dataset} & Piano-Roll \\
        \hline
    \end{tabular}
    \caption{Overview of symbolic, deep learning based music generation models, their architectures, and control mechanisms, fine-grained control, multitrack capabilities, dataset, and evaluation methods. A more complete Overview
    is found in \cite{Ji_Yang_Luo_survey_symbolic_2024}}
    \label{tab:music_models}
\end{table}


\section{Representation and Format}\label{section:representation}
The choice of representation of data is a crucial aspect of music generation. First, the choice of audio or symbolic music has consequences for data availability, dataset size, context of the output, and what features can be controlled. Second, tokenization, the method in which symbolic or audio data are chunked and ingested into the model is an important aspect, with tradeoffs to consider for different generative tasks and goals. 


\subsection{Symbolic Music vs Audio}\label{section:symbolic_audio}
Music can be represented digitally in two ways, either as an audio rendition or symbolically as a set of instructions. Working with different representations comes with various drawbacks and benefits for music generation. There are different types of symbolic representations of music, but the most common consist of discrete sequences of musical elements such as pitch or duration.  Working with audio theoretically gives access to all audible qualities of music, including detailed information on instrumental timbre or acoustic settings. In symbolic music, this is restricted to pitch, duration, and instrumentation, which sometimes is extended to include formatting information such as bar lines, etc. 
Symbolic data is also far less available than audio data. Many symbolic music datasets are created by compiling hand-transcribed music. High-quality automatic transcription remains an unsolved issue.\cite{Ji_Yang_Luo_survey_symbolic_2024}\cite{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024}  However, symbolic music gives more direct access to many higher-level musical features such as chord progressions, melodies, and instrumentation. When working with audio, these features have to be extracted first, requiring additional processing steps that are prone to inaccuracies. 
Another consideration to take into account is size: raw audio is significantly larger than a corresponding digital score. In addition, rendered audio is difficult to edit once generated. 


\subsection{Tokenisation}\label{section:tokenization}
Sequences are typically transformed into tokens, a numerical representation of data, to be handled by a machine learning algorithm. Audio-based music generation uses tokenization to condense audio while retaining its semantic content. Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} uses a variational autoencoder\cite{Kingma_Welling_2014} with a discretizing bottleneck (VQ-VAE) to create tokens from audio. Musicgen \cite{copet2023simple} tokenizes audio using the previously developed Encodec model for audio compression which similarly to VQ-VAE, learns a highly condensed discrete representation of audio \cite{Défossez_2023_encodec}. These condensed encodings are crucial for generative modeling on audio. In symbolic music depending on the representation a similar teqnique is used to encode piano-rolls for symbolic music diffusion. \cite{Min_Jiang_Xia_Zhao_polyffusion_2023}\cite{Zhu_Liu_Jiang_Zheng_texture_2024}).

\subsection{Symbolic Tokenisation} \label{section:symbolic_tok}
Symbolic music can be represented in different ways, as text (i.e the ABC notation), piano-roll, graph and sequence. The most widely used representation however is the event-based MIDI-representation and this is also reflected in the tokenisation techniques. In symbolic music tokenizers are feature extracters, and they extend  extends the standard MIDI-vocabulary with additional tokens that help models better capture different aspects of the music.\cite{Fradet_Briot_Chhel_2021}. In table \ref{bigtable} the tokenisation approach of different symbolic models is summarized.\\
\textbf{MIDI-Like} tokenization closely emulates the MIDI vocabulary, translating a MIDI file into a single stream of tokens such as note-on, note-off. The \textbf{MMM} tokenizer is designed to aid track inpainting. \textbf{REMI} \cite{Huang_Yang_remi_pop_transformer_2020} tokenization expands on MIDI-based tokenization with tokens for duration, bar and position, designed to help capture recurring musical patterns. \textbf{REMI+} \cite{Rütte_figaro_2023} tokenization extends \textbf{REMI} with an instrument token to better encode multi-instrument tracks. The PerTok tokenizer designed by Lemonaid\footnote{https://www.lemonaide.ai/}  encodes micro timings and offsets, designed to capture the full spectrum of rhythm in musical performances.\\

These extensions come at a cost: The resulting sequences of tokens can become very long, which adversely affects the model\cite{Ji_Yang_Luo_survey_symbolic_2024}. Ongoing attempts are made to condense token sequences, such as compound words or nested tokens.\cite{Ryu_Dong_nested_2024}. Dong et al.\cite{Dong_Chen_MMT_Kirkpatrick_2023} combine six different MIDI-like events (type, beat, position, pitch, duration, instrument) into single tokens. Hsiao et al.\cite{compound_word_Hsiao_Liu_Yeh_Yang_2021} differentiate between token types and group neighboring tokens into compound words, resulting in significantly shorter sequences (about 50\% compared to individual tokens). 

This thesis uses a third approach: Byte Pair Encoding (BPE) \cite{Sennrich_Haddow_Birch_BPE_2016}. BPE is used widely in language modeling, including the GPT series of models\cite{Radford_Wu_Child_Luan_gpt2_2019} and has been successfully applied to symbolic music generation.\cite{Fradet_Gutowski_Chhel_Briot_2023} The approach is simple: the most common token-pairs of the dataset are repeatidly combined into new combined tokens until the total amount of unique tokens reaches a preset vocabulary size. This approach works independently of token types and semantic content of the tokens and allows for very flexible scaling of the vocabulary. As seen in figure \ref{fig:tok_compare}, this drastically shortens the sequence length by about a third ($mean_{individual}=47976, mean_{bpe}=14519$), which in turn improves both the quality and efficiency of the model. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{IMAGES/scatter_pre_post_tok.png} 
    \caption{Scatterplot of sequence length before and after BPE tokenisation}
    \label{fig:tok_compare}
\end{figure}


\section{Control} \label{section:control}
Control is an essential aspect of any generative model. Without control, even the best generative models producing beautiful music would be of very limited real-world use. Control allows generative AI tools to become proper collaborative systems, and generate for a wide array of scenarios. In music generation control covers essentially all musical parameters. Parameters vary by representation, symbolic music for instance leaves very little room for any type of timbre control (aside from instrument selection). 
“Raw” audio models such as Stable Audio (Evans et al., 2024) can for instance be controlled for acoustic settings (i.e jazz music playing in a busy restaurant, in a \textit{large cathedral}, or \textit{through an intercom}), something that is simply not represented in symbolic music. 
For musical parameters represented in symbolic music, there are different approaches to classifying them. 
One can differentiate between global and local features (discussed in the appendix \ref{feature_cat}) \cite{Van_Kranenburg_Volk_Wiering_2013}, 
deep vs surface-level features \cite{Blacking_1971}, high-level vs low-level features \cite{Tan_Herremans_2020} and global vs fine-grained or time-varying features. 

In the context of this thesis we differentiate between global features, and time-varying features \cite{Rütte_figaro_2023}. What is time-varying or global is highly context dependent, a piece may have one time-signature and tempo as is assumed in \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} or it may vary over time as is suggested in \cite{Rütte_figaro_2023} or \cite{Huang_Yang_remi_pop_transformer_2020}. Other time varying controls could be chords \cite{Rütte_figaro_2023}\cite{Wu_Donahue_musicontrolnet_2023}\cite{Lan_Hsiao_Cheng_Yang_musicongen_2024}\cite{Min_Jiang_Xia_Zhao_polyffusion_2023}, melody \cite{copet2023simple}\cite{Min_Jiang_Xia_Zhao_polyffusion_2023} or texture \cite{Min_Jiang_Xia_Zhao_polyffusion_2023}. For the target application in an MACT - game, time-varying controls are necessary to provide a change in music that triggers a change in the patients improvisation.


\subsection{Rhythmic control} \label{section:rhytmic_weight}
The types of control exercised over rhythmic components varies by representation as discussed in \ref{representation}. In CocoMulla \cite{Lin_cocomulla_2024} generated audio is controlled with drum tracks and a piano-roll. Similarly in JASCO\cite{Tal_jasco} drum-audio is used for conditioning. In MusicConGen.\cite{Lan_Hsiao_Cheng_Yang_musicongen_2024}control for rhythm is added through tracking beats and downbeat. MusicControlNet\cite{Wu_Donahue_musicontrolnet_2023} adds beat and downbeat conditioning to an audio diffusion model.
For symbolic systems control of tempo and meter is relatively common \cite{Rütte_figaro_2023}, \cite{Huang_Yang_remi_pop_transformer_2020}, \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023}. Time-varying control over rhythm is often deployed through note-density (both vertical and horizontal)\cite{Rütte_figaro_2023},\cite{Huang_rule_diffusion_2024}. Another approach \cite{Zhu_Liu_Jiang_Zheng_texture_2024} involves passing the piano-roll as factor to guide the diffusion process. In Polyffusion\cite{Min_Jiang_Xia_Zhao_polyffusion_2023} Min et al successfully encode texture, that is disentangled into harmony and rhythm using a pre-trained variational auto-encoder \cite{Wang_vae_chord_rhythm_2020}. Herremans et al \cite{Tan_Herremans_2020} control rhythm through the high-level feature arousal, that is disentagled using a variational autoencoder into rhythm and note-density. 


\subsection{Inner Metric Analysis} 
Using a variational auto-encoder to disentangle rhythmic descriptions into lower level features is an option, but introduces additonal complexity of training a second model, and combining both to be used for inference. Instead, we use Inner Metric Analysis (IMA) to create metric profiles of a sequence, and use theese profiles as guiding features. Inner Metric Analysis identifies strong and week pulses and their periods within note onset in symbolic music.\cite{} It is used to identify \textit{local meters} as opposed to \textit{outer meter} given by time-signatures, which can be useful in the study of syncopation \cite{Bemman2024}\cite{Volk2008Syncopation}. IMA is also used in the classification of dance-music \cite{Chew_Volk_Lee_Dance_metric_weight_2005}, automatic detection of meter \cite{Haas_Volk_2016} and music retrieval \cite{Volk_Garbers_VanKranenburg_Wiering_Grijp_Veltkamp_2009}. 

A local meter is a set of evenly spaced onsets with a minimum length of three, not able to be extended forward or backwards in time, and not contained within any other local meter. (see figure \ref{fig:ima_all}). Let $M(l)$ be the set of local meters with a length of at least l. The parameter $p$ is variable and determines how much the length of a local meter influences its weight. Intuitivly, longer and more established local meters should carry more weight. This is given by $k_{m}^p$. The weight of an onset $W_{l,p}(o)$ is defined as the weighted sum of the local meters.  
The metric weight is calculated as follows \cite{Volk2008Syncopation}.  

\begin{equation}
    W_{l,p}(o) = \sum_{m \in M(l):o \in m}k_{m}^{p}
\end{equation} 
Spectral weight is a variation of the metric weight, that consider the extension of each local meter: $ext(m)$. The red triangles in figure \ref{fig:ima_all} are and example of extensions. The calculation is similar, but it assigns a weight to each time point (instead of only to onsets) and considers the extensions. This feature is less sensitive to local changes.

\begin{equation}
    SW_{l,p}(t) = \sum_{m \in M(l):t \in ext(m)}k_{m}^{p}
\end{equation}

In the context of MACT, inner metric analysis is interesting because it can indicate rhythmic complexity, which in turn influences ease of tapping along/following \cite{Volk2008Syncopation} and rhythmic entrainment. This could be extended to generate music that is more difficult to follow to allow for difficulty adjustment in the context of an MACT game. 

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{IMAGES/IMA1.JPG}
        \caption{Single local meter and its pulses (A) from the melody "Twinkle, Twinkle Little Star"}
        \label{fig:ima1}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{IMAGES/IMA2.JPG}
        \caption{10 local meters and their pulses produced by IMA of a syncopated variation of "Twinkle, Twinkle Little Star"}
        \label{fig:ima2}
    \end{subfigure}
    
    \vspace{0.5cm} % Adjust spacing between rows

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{IMAGES/IMA3.JPG}
        \caption{Metric weight profile of syncopated "Twinkle, Twinkle Little Star"}
        \label{fig:ima3}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{IMAGES/IMA4.JPG}
        \caption{Spectral weight profile of syncopated "Twinkle, Twinkle Little Star"}
        \label{fig:ima4}
    \end{subfigure}

    \caption{Visualisation of metric weight analysis \cite{Bemman2024}}
    \label{fig:ima_all}
\end{figure}


\section{Implementation of control} \label{section:addingcontrol}
Methods of enabling control in a generative models can be split into four approaches. 1: Choice of architecture, 2: training data, 3: fine-tuning and 4: post-hoc guidance.

\subsection{Control through architecture}
The choice of architecture lends itself to different types of control. Transformers are next token predictors, that predict based on the prior sequence. The default training paradigm allows for control/conditioning with a user defined musical (or audio) sequence. This is true for both audio based models such as MusicGen \cite{copet2023simple}, Jukebox \cite{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020} and MusicLM \cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023} as well as symbolic music models such as MMT \cite{Dong_Chen_MMT_Kirkpatrick_2023}, MusicTransformer \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018} and MusicBERT \cite{Zeng_Tan_Wang_MUSICBERT_2021}. Diffusion models are quite flexible compared to transformers, the same model can be used for inpainting, continuation and - depending on the representation - melody and accompaniment generation through masking.\cite{Min_Jiang_Xia_Zhao_polyffusion_2023}\cite{Rombach_Blattmann_Lorenz_Esser_Ommer_2022}Transformers have to be explicitly trained for theese tasks. 

\subsection{Control through training data}
Joint training of a model with the desired control, is the most common and robust method of enabling control in a generative model. MusicGen\cite{copet2023simple}, a recent text-to-music (audio) transformer is trained on 20000 hours of licensed music from shutterstock and pond5 \footnote{https://www.shutterstock.com/music and https://www.pond5.com/} which includes textual descriptions and tags for genre, tempo, and other factors such as instrumentation. Control is achieved through the joint training of a text description and music. An example description is provided below: 

\textit{Inspirational dramatic background music! Perfect for trailer, background, advertising, historical film, movie about superheroes, teaser and many other projects!}\footnote{ https://www.pond5.com/royalty-free-music/item/95908062-inspiring-dramatic-epic-background-cinematic-music
}

Text-based control, while user-friendly and accessible to non-musicians, is inherently vague. Levels of detail and choice of words vary widely by dataset, even with standardized tags such as genre and tempo. This is also true of specialized music-text datasets such as MusicCaps \cite{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023},\cite{Lee_Doh_Jeong_2023_subjectivity_musiccaps}. For this reason, the creators of MusicGen \cite{copet2023simple} add melody conditioning alongside text conditioning and train their model jointly with the chromagram of the melody alongside the text. 

In MusicGenStyle \cite{Rouard_Adi_Copet_Roebel_Défossez_musicgenstyle_2024} perform classifier-free guidance to add style conditioning to MusicGen. They train a music-style encoder that transforms a random subsample of a given reference audio track into tokens that are combined with the embeddings of the text-description. Both the style tokens and text tokens are provided as prefix to the model. The conditioner and the MusicGen transformer are trained jointly on the entire dataset. 
The creators of FIGARO\cite{Rütte_figaro_2023} enable fine grained control over instrumentation, note density, average pitch and volume on a bar-by-bar basis, in a symbolic music generator through joint conditioning while training. 

\subsection{Adding control through fine-tuning}

Both the melody conditioning of MusicGen \cite{copet2023simple} and the style conditioning of MusicGenStyle \cite{Rouard_Adi_Copet_Roebel_Défossez_musicgenstyle_2024} retrain the entire MusicGen model on the entire dataset which comes at considerable cost. Fine-tuning or transfer learning is another method through which models can be trained but at considerably smaller cost and using less data. This is particularly useful and widely used in the language domain to adjust large language models for niche use-cases, where the available data may simply not be sufficient to train a large model from scratch. In the examples of MusicGen and MusicGenStyle the availability of data was not a limiting factor since the controlling elements, melody and style can be inferred from the training data. However, fine-tuning may achive additional control at a lower cost. 

MusiConGen \cite{Lan_Hsiao_Cheng_Yang_musicongen_2024} is a fine-tuned variation of MusicGen which adds rhythm and chord control. They propose the jump-finetuning mechanism, where the original model with 1.5 Billion parameters and 48 self-attention layers, is split model into blocks consisting of 4 self-attention layers. They refine the first layer of each block, freezing the remaining layers. Additionally, they apply adaptive in-attention to the first 9 blocks, where the output of the transformer is augmented with copies of the original condition. As a result, only a quarter of the original parameters are tunable, which enables training on consumer GPUs on just 250 hours of music sourced from YouTube (as opposed to 20000 hours).  In Coco-Mula \cite{Lin_cocomulla_2024} the authors adjust a LLAMA adapter with just 4\% of parameters, keeping all original MusicGen parameters frozen, and training only the adapter on a small dataset of 300 songs to add drum and chord conditioning. 

MuseBarControl \cite{Shu_Xu_Musebarcontrol_2024} is a fine-tuned version of MuseCoco\cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} which extends the global controls with time-varying bar level control for music-generation. They compare several approaches. In the first they augment the prompt (which is generated from text) with additional tokens for bar-wise control of chords, and adjust the loss function to incoperate that.  In the second approach they introduce two novel methods, first, they pre-adapt the new parameters (introduced by the lora adapter) to a separate classification task, an auxiliary task . The model classifies whether the a section of music corresponds with the control tokens, the body of the model is trained together with a classification head (which is removed after auxiliary task training. In the third step they introduce counterfactual loss where the difference in negative log likelyhood conditioned on the original and changed attribute is maximized, which reinforces the models attention to the control. They find that the combination of the three strategies, pre-adaptation on a separate task followed with counterfactual-loss and prompt augmentation yields the strongest model. 

\subsection{Adding control through guidance}
There are also other methods that do not involve any finetuning or retraining of the original model. Adding control with additional model inputs does require at least some amount retraining, which is not always feasible, and adding many different types of control may deterioate the model performance. In these cases, guidance can be used to steer the model towards a certain output.
In SMITIN \cite{Koo_Wichern_Germain_SMITIN_2024} the authors intervene at inference time, while the trained model is generating, to guide MusicGen\cite{copet2023simple}  towards a certain goal. They explore guidance for ensuring the presence of certain instruments (piano/drums/bass/guitar) and to increase the quality/realism of the generated audio. For this,  the authors train linear probes that learn to associate the state of each transformer layer in the networke with the goal (i.e drums being present in the output). Then the attention heads are steered in the direction of the probe’s output, which increases the probability of the desired quality (drums) being present in the generated music. Guidance of transformer models has been explored in other contexts \cite{language_guide_rutte_2024}, i.e influencing truthfullness, humor, and appropriatness, with mixed results. 

In Diffusion models, the output is sampled over several steps, at each of these steps it is possible to intervene with guidance to direct the sampling towards a certain goal. In \cite{Huang_rule_diffusion_2024}, each sampling step is repeated several times, and each time the sample that follows a set of rules most closely is chosen. ControlNet \cite{Zhang_Rao_Agrawala_2023} adds spacial control to image generators allowing the guidance of image generation using sketches, poses, edges and depth maps without retraining. MusicControlNet \cite{Wu_Donahue_musicontrolnet_2023} adapts this approach to music generation adding control for time varying factors, melody, dynamics and rhythm. 


\section{Evaluation} \label{section:evaluation}
How to evaluate generated music is still an open research question. There are no standardized methods according to which evaluations happen\cite{Yin_Reuben_Stepney_Collins_2023}. In the context of music generation there are several proposed frameworks to evaluate music. Music generation literature often distinguished between "objective" and "subjective" evaluation. Despite what the name suggest objective evaluation of generated music does not (usually) claim to evaluate the aesthetic quality or beauty of the music objectivly, instead it encompasses a series of automated, statitistical methods of analysing and comparing generated music to human composed music. Subjective evaluation encompasses evaluation methods that center human judgement. \\

\subsection{Subjective Evaluation}
For subjective approaches the methods vary widely \cite{Xiong_Wang_ai_eval_methods_2023}. There are simple turing-type evaluations that test how distinguishable generated and human written music are. There are tournament style surveys, where the number of winning pieces are tallied for each approach.\cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018}\cite{Rütte_figaro_2023}
Another typical approach is using (likert) ratings along different dimensions to separate different qualities of the music. \cite{Dong_Chen_MMT_Kirkpatrick_2023}, \cite{Yu_Lu_Wang_Hu_Tan_Ye_Zhang_museformer_2022} and \cite{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024} collect likert ratings on coherence, richness, arrangement and consistency. Specifically in \cite{Dong_Chen_MMT_Kirkpatrick_2023} this takes the following form: 
coherence—“Is [the music] temporally coherent? Is the rhythm steady? Are there many out-of-context notes?”;
richness—“Is [the music] rich and diverse in musical textures? Are there any repetitions and variations? Is [the music] too boring?”; 
arrangement—“Are the instruments used reasonably? Are the instruments arranged properly?”. 
The specific questions asked vary by what exactly the project aimed to achieve. 
In \cite{Yin_Reuben_Stepney_Collins_2023} likert ratings are collected along the dimensions of stylistic success, aesthetic pleasure, repetition, melody, harmony, rhythm. Here, the questions for stylistic success are relevant due to their use of generative models to produce music in a certain style specifically classical string quartets, and classical piano improvisations. 
These evaluations are often paired with statistical hypothesis testing, to investigate relationships between the various participant ratings of the different models. An example would be: There is no difference between ModelA and ModelB on ratings of stylistic success. Or ratings of melodic success are positively correlated with ratings of aesthetic pleasure.
Finally there are expert evaluations (which can also include likert ratings) but also detailed analysis or even performance of the produced music \cite{Sturm_Ben-Tal_2016} \cite{Yin_Reuben_Stepney_Collins_2023}. \\

\subsection{Objective Evaluation}

Objectuve evaluation of generated music include model specific metrics and different musical metrics \cite{Xiong_Wang_ai_eval_methods_2023}. Model specific metrics are generic evaluations of a models success to approximate training data, these will vary depending on the model and are not indicative of stylistic success. Examples of this are Negative Log Likelyhood \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018}, Root Mean Square Error \cite{Rütte_figaro_2023} or Perplexity\cite{Rütte_figaro_2023}. Musical metrics typically involve comparing a set of generated music to a set of real music, there are plenty of musical similarity measure techniques\cite{Gurjar_Moon_similarity_2018} for a large variety of different use-cases i.e music retrieval, cover, genre and artist detection. A popular comparative metric is calculating the Kulback Leibler (KL) divergence between two datasets with respect to certain metrics i.e count of intervals or unique pitch-classes. However to obtain the divergence one has to select specific features that may only capture a subset of the desired properties. Similar issues arise with other distance metrics i.e cosine similarity, earth movers distance or maximum overlapping area. 

Especially in the audio domain, additional AI models are often used for evaluation. MusicGen \cite{copet2023simple} uses additional classifiers to generate labels for the music and calculates the KL-divergence between the generated labels. Additionally they calculate the Frachet Audio distance, a measure devised to calculate the plausibility of audio (for music enhancement purposes) compared to a large set of studio recordings\cite{Kilgour_Frachet_2019}. Finally they use the CLAP-score which compares the corresponding text description to the latent representation of the generated audio, with text-description of the generated audio with the reference audio. \cite{Elizalde_Deshmukh_Ismail_Wang_2023}

For this thesis we are interested in two factors, first the plausibility of the generated music, and second the success of the control. How the success of control is evaluated depends on what is controlled for,  Examples of controlled parameters and how they are evaluated are as follows:

\textbf{Note Density}.  (how many notes per bar). Root mean square error (RME) between generated vs target notes per bar. This is the approach to compare note density used in \cite{Rütte_figaro_2023}

\textbf{Rhythmic patterns}. Partial Similarity \cite{Volk_Garbers_VanKranenburg_Wiering_Grijp_Veltkamp_2009} between target and generated music: 

