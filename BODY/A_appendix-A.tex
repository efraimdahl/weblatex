%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Appendix example ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Appendix}
% START APPENDIX
\section{Comparing Tokenization lengths}
\label{tok_compare}
Pretokenized Sequence Lengths - Mean: 47976, Median: 43692.0, Std Dev: 23160.\\
Tokenized Sequence Lengths - Mean: 14519, Median: 13159.0, Std Dev: 7197\\

Applying the BPE tokenizer results in sequence lengths roughly 1/3 of the original sequences. The sample is taken from 1000 random music pieces from the lakh-midi dataset. 

\section{Feature categorisation}\label{feature_cat}

\textbf{Global features} in music generation encompass high-level descriptors such as genre, function, and instrumentation/orchestration. Global features also incorporate summarizing features, that are calculated from the music itself. As an example, McKay’s \cite{McKay_2004} general-purpose symbolic music classification system defines over 104 global features, 37 of them used for melody descriptors such as probability distributions of note durations, intervals, and pitch classes. These global features can then be used to infer other high-level descriptors such as genre if it's unknown, they can also support music retrieval systems \cite{Van_Kranenburg_Volk_Wiering_2013}.  \textbf{Local features} describe individual sequences such as melodic, rhythmic, or harmonic sections. They are more information-rich, which may explain their superior performance in music retrieval \cite{Van_Kranenburg_Volk_Wiering_2013}. 

Also relevant in the music generation context are \textbf{high-level} vs \textbf{low-level} features. Music generation can take high-level concepts such as genre \cite{Rouard_Adi_Copet_Roebel_Défossez_musicgenstyle_2024}, \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} or emotion \cite{Tan_Herremans_2020} \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} into account, which in turn effects more complex dynamics of concrete features. To illustrate: In Music FaderNets \cite{Tan_Herremans_2020} the authors use a variational autoencoder to disentangle the abstract concept of arousal, into several concrete features including rhythmic density, note-density, tempo and dynamic, key. This allows them to change the abstract variable arousal, which cascades into a change of underlying features. This type of disentanglement can aid in situations where labled musical data with the abstract feature is less available, it can also aid in creating more accessible and interpretable generative models.


\section{LastMinuteGig - music algorithms}\label{musicalgorithm}
This project is motivated by extending the music engine of LastMinuteGig. \cite{Chalkiadakis_2022}. The application currently employs a very simple algorithm. I will outline it below. 
\begin{itemize}
    \item{1}: Randomly choose key, rhythmic pattern, chord progression and tempo from a pool of possible values.
    \item{2}: Play corresponding percussion audio clip (depends on tempo and rhythmic pattern)
    \item{3}: When user plays the button trigger the guitar sound of the current chord.
    \item{4}: After 8 bars - make a random change (to rhythm, tempo, pause)
\end{itemize}


The following is a description of an improved algorithm utilizing generated music by RhythmLang
\begin{itemize}
    \item{1}: Create $n=100$ musical pieces with different control settings
    \item{2}: For each piece
    \item{3}: Extract chords from prompt (assuming high control success) - save mapping $m1$
    \item{4}: Extract change of rhythmic pattern (or tempo or meter or instrumentation since these are already controllable in the musiclang model) - save mapping
    \item{5}: Render symbolic music to audio
\end{itemize}

During game play
\begin{itemize}
    \item{1}: Load random audio clip and mappings.
    \item{2}: Play associated guitar chords when player taps.
    \item{3}: Register changes in tapping on musical changes.
    \item{4}: Repeat when audio clip has finished playing.
\end{itemize}


\section{MIDI Bass Instruments}\label{midi-bass}
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \textbf{Instrument Name} & \textbf{MIDI Channel} \\
        Electric Bass (Finger)  & 34 \\
        Electric Bass (Pick)    & 35 \\
        Fretless Bass           & 36 \\
        Slap Bass 1            & 37 \\
        Slap Bass 2            & 38 \\
        Synth Bass 1           & 39 \\
        Synth Bass 2           & 40 \\
        Acoustic Bass          & 33 \\
        Contrabass             & 43 \\
        Cello                  & 46 \\
        Trombone               & 57 \\
        Tuba                   & 58 \\
    \end{tabular}
    \caption{MIDI Instrument Mapping}
    \label{tab:midi_instruments}
\end{table}



\section{Notes Comparing Symbolic Music Generators and their evaluation methods}
\label{section:compare_sym}
\begin{itemize}

\item DeepBach \cite{Hadjeres_Pachet_Nielsen_2017} - RNN - inpainting. Evaluation Turing
\item FolkRNN \cite{Sturm_Ben-Tal_2016} - RNN - control for meter and mode - Expert Evaluation + Performance Practice
\item MusicTransformer \cite{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018} - Transformer - Evaluation: Subjective - Tournament Style between different generated and natural music. Objective: Validation NLL
\item MidiNet\cite{midinet} - GAN - Control for Chords/Priming melody.
 Human (how pleasing, how real, and how interesting)

\item Polyfussion \cite{Min_Jiang_Xia_Zhao_polyffusion_2023} - Diffusion Model - supports inpainting, interpolation, melody/accompaniment generation, control for chord progression, texture. Subjective Evaluation Questionnaire for naturalness, creativity, musicality. Objective Control success. 

\item FIGARO \cite{Rütte_figaro_2023} - Transformer Model - bar-wise control for chords, instrumentation, time-signature, note-density, mean-pitch. 
Evaluation: Perplexity (improvement over NLL for sequences of different length), Discription Fidelity (i.e accuracy in regards binary controls instruments, chords, time-signature). Macro Overlapping Area - comparison of feature histograms. Normalized Mean Root Square Area for note-density. Cosine similarity for chroma (melodic) and groove (rhythmic) feature vectors. 
Ablation study - effect of turning off controls.
Extensive Subjective evaluation:  7569 comparisons by 691 participants - tournament style.

\item Multi Track Music Transformer  (MMT) \cite{Dong_Chen_MMT_Kirkpatrick_2023}  - Transformer - Instrument control. Comparison of different tokenisation techniques, REMI+ and Compound Tokens. Compound tokens are more condensed and the generated samples are longer, achieves significant speedups and reduces memory usage (2.6 * MMM, or 3.5 * REMI+). Objective evaluations: Inference time, pitch class entropy, scale consistency, groove consistency, 
Human evaluation (90 comparisons by 9 participants) on Coherence Richness Arrangement Overall.

Additional: Analysis of self attention as explanation avenue, which notes are most important. 

\item REMI pop music transformer - \cite{Huang_Yang_remi_pop_transformer_2020} - Transformer - continuation, control local control over chord and tempo

\item MuseNet \cite{Christine_2019} - Transformer -,Instrument control, style control. No evaluation, only showcase.

\item MMM \cite{Ens_Pasquier_2020_MMM} - Transformer model - inpainting, instrument control, note-density.
Introduce novel representation
No Rigorous Evaluation 

\item SymPAC \cite{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024}  - Transformer - Control for Chords, structure, instrumentation, single notes.
Train with both symbolic and transcribed audio data.
Fine grained control. Constrained generation with a Finite State Machine. 
Comparison of three separate models trained on three datasets.
\textbf{Evaluation} of controlability with KL-divergence on different controlled features over chords, structure, and individual notes. - KL divergence decreases with dataset size. 800 samples are generated and compared against a validation set of 3000 songs. 
Subjective evaluation (12 expert participants MIR researchers and music producers) on parameters of Coherence Richness Arrangement Structure 

\item MuseCoco \cite{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023} - Transformer - Control via text for following attributes instrumentation, ambitus, rhythm (intensity and “dancability”), number of bars, time signature, key, tempo, duration, artist, emotion, genre. All of these controls are global controls, a description is converted into a list of  attributes which is then used for generation. 
Combination of many datasets. Creation of text descriptions from attribute list with data from the dataset, and ChatGPT. 
Evaluation: Objective - text to attribute list. 
Subjective evaluation 19 participants with at least basic music knowledge - questions to Musicality, Controlability (adherence of sample to music description), Overal Impression. Comparison with LLM generated music (with no special training for music generation)

\item MBD \cite{Shu_Xu_Musebarcontrol_2024} Extension of MuseCoco for time varying chord controls. Counterfactual Loss and Auxiliary task training improve controllability. 

\item{Museformer} \cite{Yu_Lu_Wang_Hu_Tan_Ye_Zhang_museformer_2022} - Transformer - No controls. Goal improve long-term structure with fine and coarse attention. Captures structure well. 
Objective Evaluation: Perplexity: prediction accuracy of next token, Similarity Error the error between the similarity distribution of training data and generated
music 
Subjective Evaluation 10 Participants Musicality, Long Term Structure, Short Term Structure.
Ablation study - evaluate objective effect of coarse and fine-grained attention. + Case study and detailed look at model.

\item{NMT} \cite{Ryu_Dong_nested_2024} - Transformer improve longterm structure and reduce sequence length through compound tokens. Application to both symbolic and audio tokens. 
Cross attention vs self-attention comparison 
Evaluation: FAD, CLAP, KL and NLL over audio tokens. NLL over symbolic tokens. 
Subjective Evaluation Coherence, Richness, Consistency, Overall 29 participants. 8 selected prompts, 4 different continuations with REMI, Compound Word and 2 NMT variations.


\item{FTG - Fine Grained Texture Control} - Diffusion - control over texture, rhythm and chords. 

\item{Fader Nets}\cite{Tan_Herremans_2020} - VAE - Control over rhythm, arousal, 
Idea: develop a "fader" representing a high-level abstract feature i.e arousal. Arousal is disentangled using a VAE into lower level features (i.e rhythmic density).

Evaluation of the influence of latent features is on generated (style transfer) music.:
Consistency, Restrictiveness (one latent dimension does not influence other musical features), Linearity (linear change in latent feature - linear change in musical feature)
Subjective listening test to indicate success of arousal shift -> 48 participants, evaluate agreement with arousal direction. 

\item{NDRD} Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion.
Guidance of diffusion sampling with non-differentiable rules. 
\end{itemize}

