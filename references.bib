%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%		~~~~ Bibliography ~~~~
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Methods to get the information:
%       - Use the DOI of an article with doi2bib.org
%       - Use Google scholar to get .bib citations


@article{LA_top10_US,
  title   = {Top Value Added Chemicals from Biomass: Volume I -- Results of Screening for Potential Candidates from Sugars and Synthesis Gas},
  author  = {Werpy, T and Petersen, G},
  doi     = {10.2172/15008859},
  journal = {},
  place   = {United States},
  year    = {2004},
  month   = {8}
}

@inproceedings{Rogers_Weber_2019,
  address      = {New York, NY, USA},
  series       = {AM ’19},
  title        = {Audio Habits and Motivations in Video Game Players},
  isbn         = {978-1-4503-7297-8},
  url          = {https://doi.org/10.1145/3356590.3356599},
  doi          = {10.1145/3356590.3356599},
  abstractnote = {Game music is increasingly being explored in terms of empirical effects on players, but we know very little about how players actually perceive and use background music in games, and why. We conducted a survey (N=737) to gain an understanding of players’ in-the-wild audio habits and motivations, which can inform future research and industry practices regarding game audio design. The results indicate a wide variance in players’ estimation of the importance of music in games. Further, we identify and provide evidence for the comparatively new multitasking phenomenon: a substantial number of players who do not listen to games’ provided background music, often in favour of additional/parallel media usage. Based on these findings, we discuss implications for game audio design, ground existing common knowledge with empirical support, and delineate future research directions.},
  booktitle    = {Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound},
  publisher    = {Association for Computing Machinery},
  author       = {Rogers, Katja and Weber, Michael},
  year         = {2019},
  month        = sep,
  pages        = {45–52},
  collection   = {AM ’19}
}

 @mastersthesis{Chalkiadakis_2022,
  address = {Utrecht, NL},
  title   = {Developing and evaluating a Musical Attention Control Training computer game application.},
  url     = {https://studenttheses.uu.nl/bitstream/handle/20.500.12932/500/Master%20Thesis%20-%20Developing%20and%20evaluating%20a%20Musical%20Attention%20Control%20Training%20computer%20game%20application%20%285892252%29.pdf?sequence=1\&isAllowed=y},
  school  = {Utrecht University},
  author  = {Chalkiadakis, Ermis},
  year    = {2022},
  month   = jan
}

@inproceedings{copet2023simple,
  title     = {Simple and Controllable Music Generation},
  author    = {Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D{\'e}fossez},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year      = {2023},
  url       = {https://openreview.net/forum?id=jtiQ26sCJi},
  doi       = {10.48550/arXiv.2306.05284}
}

 @article{Agostinelli_Denk_Borsos_Engel_Verzetti_Caillon_Huang_Jansen_Roberts_Tagliasacchi_et_al._2023,
  title     = {MusicLM: Generating Music From Text},
  url       = {http://arxiv.org/abs/2301.11325},
  doi       = {10.48550/arXiv.2301.11325},
  journal   = {arXiv},
  number    = {arXiv:2301.11325},
  publisher = {arXiv},
  author    = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zalán and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
  year      = {2023},
  month     = jan
}


 @article{Ferdinand_Meyen_2024,
  title   = {“Verknallt in einen Talahon”: So reagiert der Talahon-Produzent auf die Rassismus-Vorwürfe},
  url     = {https://www.br.de/radio/bayern2/sendungen/zuendfunk/verknallt-in-einen-talahon-produzent-rassismus-vorwuerfe-charts-udio-ki-100.html},
  journal = {Bayern 2 Zuendfunk},
  author  = {Ferdinand Meyen, Bayerischer Rundfunk},
  year    = {2024},
  month   = aug
}

 @article{Stassen_2023,
  title        = {"There are now 120,000 new tracks hitting music streaming services each day"},
  url          = {https://www.musicbusinessworldwide.com/there-are-now-120000-new-tracks-hitting-music-streaming-services-each-day/},
  abstractnote = {A total of 10.08 million new songs uploaded to the likes of Spotify, YouTube Music and other music streaming services in the first three months of the year alone…},
  journal      = {Music Business Worldwide},
  author       = {Stassen, Murray},
  year         = {2023},
  month        = may
}

 @article{Stassen_2024,
  title   = {Suno, with a \$500m valuation, has admitted training its AI on copyrighted music},
  url     = {https://www.musicbusinessworldwide.com/suno-with-a-500m-valuation-has-admitted-training-its-ai-on-copyrighted-music-it-just-named-timbaland-as-a-strategic-advisor1/},
  journal = {Music Business Worldwide},
  author  = {Stassen, Murray},
  year    = {2024},
  month   = apr
}

 @article{France-Presse_2016,
  title        = {First recording of computer-generated music – created by Alan Turing – restored},
  issn         = {0261-3077},
  url          = {https://www.theguardian.com/science/2016/sep/26/first-recording-computer-generated-music-created-alan-turing-restored-enigma-code},
  abstractnote = {Researchers restore 1951 recording generated on machine built by computer scientist famous for breaking Enigma code},
  journal      = {The Guardian},
  author       = {France-Presse, Agence},
  year         = {2016},
  month        = sep
}

 @book{Hiller_Isaacson_1959,
  title        = {Experimental music; composition with an electronic computer},
  callnumber   = {MT41 .H58},
  url          = {http://archive.org/details/experimentalmusi00hill},
  abstractnote = {“Illiac suite for string quartet”: p. 182-197},
  publisher    = {New York, McGraw-Hill},
  author       = {Hiller, Lejaren and Isaacson, Leonard M. (Leonard Maxwell)},
  year         = {1959}
}

 @book{Xenakis_1992,
  title     = {Formalized Music Thought and Mathematics in Composition},
  url       = {https://monoskop.org/images/7/74/Xenakis_Iannis_Formalized_Music_Thought_and_Mathematics_in_Composition.pdf},
  publisher = {Pendragon Press},
  author    = {Xenakis, Iannis},
  year      = {1992}
}

 @book{Cope_1989,
  title     = {New directions in music},
  isbn      = {978-0-697-03342-0},
  url       = {http://archive.org/details/unset0000unse_x4d7},
  publisher = {Dubuque, Iowa : W.C. Brown},
  author    = {Cope, David},
  year      = {1989}
}

 @article{Featherstone_2017,
  title        = {Introducing the next generation of music makers},
  issn         = {0261-3077},
  url          = {https://www.theguardian.com/small-business-network/2017/aug/29/computer-write-music-jukedeck-artificial-intelligence},
  abstractnote = {The founders of Jukedeck, a startup that uses AI to write music, say musicians have been surprisingly supportive of the technology},
  journal      = {The Guardian},
  author       = {Featherstone, Emma},
  year         = {2017},
  month        = aug
}
 
@inproceedings{midinet,
  address   = {Suzhou, China},
  title     = {MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation},
  url       = {http://arxiv.org/abs/1703.10847},
  doi       = {10.48550/arXiv.1703.10847},
  booktitle = {18th International Society for Music Information Retrieval Conference (ISMIR)},
  author    = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
  year      = {2017},
  month     = jul
}

 @inproceedings{Hadjeres_Pachet_Nielsen_2017,
  title     = {DeepBach: a Steerable Model for Bach Chorales Generation},
  url       = {http://arxiv.org/abs/1612.01010},
  doi       = {10.48550/arXiv.1612.01010},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  publisher = {PMLR 70:1362-1371},
  author    = {Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
  year      = {2017},
  month     = jun
}

 @inbook{Sturm_Ben-Tal_2016,
  address   = {Cham},
  title     = {Folk the Algorithms: (Mis)Applying Artificial Intelligence to Folk Music},
  isbn      = {978-3-030-72115-2},
  url       = {https://link.springer.com/10.1007/978-3-030-72116-9_16},
  doi       = {10.1007/978-3-030-72116-9_16},
  publisher = {Springer International Publishing},
  author    = {Sturm, Bob L. T. and Ben-Tal, Oded},
  editor    = {Miranda, Eduardo Reck},
  year      = {2016},
  pages     = {423–454}
}

 @inproceedings{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017,
  title     = {Attention Is All You Need},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  booktitle = {NIPS’17: Proceedings of the 31st International Conference on Neural Information Processing Systems},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year      = {2017}
}

@article{Dhariwal_Jun_Payne_Kim_Radford_Sutskever_2020,
  title     = {Jukebox: A Generative Model for Music},
  journal   = {arXiv},
  url       = {http://arxiv.org/abs/2005.00341},
  doi       = {10.48550/arXiv.2005.00341},
  number    = {arXiv:2005.00341},
  publisher = {arXiv},
  author    = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year      = {2020},
  month     = apr
}

 @misc{Christine_2019,
  title  = {MuseNet},
  url    = {openai.com/blog/musenet},
  author = {Christine, Payne},
  year   = {2019},
  month  = apr
}

 @misc{Suno_AI,
  url          = {https://suno.com/about},
  abstractnote = {We are building a future where anyone can make great music. No instrument needed, just imagination. From your mind to music.},
  language     = {en},
  key          = {suno.com}
}

 @article{Plut_Pasquier_2020,
  title   = {Generative music in video games: State of the art, challenges, and prospects},
  volume  = {33},
  issn    = {1875-9521},
  doi     = {10.1016/j.entcom.2019.100337},
  journal = {Entertainment Computing},
  author  = {Plut, Cale and Pasquier, Philippe},
  year    = {2020},
  month   = mar,
  pages   = {100337}
}

 @article{Worrall_Collins_2024,
  title   = {Considerations and Concerns of Professional Game Composers Regarding Artificially Intelligent Music Technology},
  volume  = {16},
  rights  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn    = {2475-1502, 2475-1510},
  doi     = {10.1109/TG.2023.3319085},
  author  = {Worrall, Kyle and Collins, Tom},
  year    = {2024},
  month   = sep,
  pages   = {586–597},
  journal = {IEEE Transactions on Games}
}

 @article{musicwellbeing_agres_2021,
  title     = {Music, Computing, and Health: A Roadmap for the Current and Future Roles of Music Technology for Health Care and Well-Being},
  volume    = {4},
  issn      = {2059-2043},
  doi       = {10.1177/2059204321997709},
  journal   = {Music \& Science},
  publisher = {SAGE Publications Ltd},
  author    = {Agres, Kat R. and Schaefer, Rebecca S. and Volk, Anja and van Hooren, Susan and Holzapfel, Andre and Dalla Bella, Simone and Müller, Meinard and de Witte, Martina and Herremans, Dorien and Ramirez Melendez, Rafael and Neerincx, Mark and Ruiz, Sebastian and Meredith, David and Dimitriadis, Theo and Magee, Wendy L.},
  year      = {2021},
  month     = jan,
  pages     = {2059204321997709}
}


 @article{Park_Kim_2021,
  title    = {Dual-Task-Based Drum Playing with Rhythmic Cueing on Motor and Attention Control in Patients with Parkinson’s Disease: A Preliminary Randomized Study},
  volume   = {18},
  issn     = {1660-4601},
  doi      = {10.3390/ijerph181910095},
  number   = {19},
  journal  = {International Journal of Environmental Research and Public Health},
  author   = {Park, Jin-Kyoung and Kim, Soo Ji},
  year     = {2021},
  month    = sep,
  pages    = {10095},
  language = {eng}
}

 @article{Martin-Moratinos_Bella-Fernández_Blasco-Fontecilla_2023,
  title   = {Effects of Music on Attention-Deficit/Hyperactivity Disorder (ADHD) and Potential Application in Serious Video Games: Systematic Review},
  volume  = {25},
  issn    = {1439-4456},
  doi     = {10.2196/37742},
  journal = {Journal of Medical Internet Research},
  author  = {Martin-Moratinos, Marina and Bella-Fernández, Marcos and Blasco-Fontecilla, Hilario},
  year    = {2023},
  month   = may,
  pages   = {e37742}
}

 @article{Pasiali_LaGasse_Penn_2014,
  title   = {The effect of musical attention control training (MACT) on attention skills of adolescents with neurodevelopmental delays: a pilot study},
  volume  = {51},
  issn    = {0022-2917},
  doi     = {10.1093/jmt/thu030},
  number  = {4},
  journal = {Journal of Music Therapy},
  author  = {Pasiali, Varvara and LaGasse, A. Blythe and Penn, Saundra L.},
  year    = {2014},
  pages   = {333–354}
}

 @article{van_Alphen_Stams_Hakvoort_2019,
  title   = {Musical Attention Control Training for Psychotic Psychiatric Patients: An Experimental Pilot Study in a Forensic Psychiatric Hospital},
  volume  = {13},
  issn    = {1662-4548},
  doi     = {10.3389/fnins.2019.00570},
  journal = {Frontiers in Neuroscience},
  author  = {van Alphen, R. and Stams, G. J. J. M. and Hakvoort, L.},
  year    = {2019},
  pages   = {570}
}

 @inproceedings{Malandro_2023,
  address   = {San Francisco, USA},
  title     = {Composer’s Assistant: An Interactive Transformer for Multi-Track MIDI Infilling},
  url       = {http://arxiv.org/abs/2301.12525},
  doi       = {10.48550/arXiv.2301.12525},
  note      = {arXiv:2301.12525 [cs]},
  booktitle = {2024 International Society for Music Information Retrieval},
  publisher = {ISMIR},
  author    = {Malandro, Martin E.},
  year      = {2023},
  month     = jul
}

 @article{Tencer_2024,
  title    = {New AI-powered ‘instant’ music-making app Udio raises \$10m; launches with backing from will.i.am, Common, UnitedMasters, a16z},
  url      = {https://www.musicbusinessworldwide.com/new-ai-powered-instant-music-making-app-udio-raises-10m-launches-with-backing-from-will-i-am-common-unitedmasters-a16z/},
  journal  = {Music Business Worldwide},
  author   = {Tencer, Daniel},
  year     = {2024},
  month    = apr,
  language = {en-US}
}

 @misc{Kaba_River_Perry_2024,
  title  = {Demand for Jury Trial},
  url    = {https://storage.courtlistener.com/recap/gov.uscourts.mad.272063/gov.uscourts.mad.272063.1.0.pdf},
  number = {1:24-cv-11611},
  author = {Kaba, Moez M. and River, Mariah N. and Perry, Alexander R.},
  year   = {2024},
  month  = jun
}

 @inproceedings{Duan_Suri_Mireshghallah_Min_Shi_Zettlemoyer_Tsvetkov_Choi_Evans_Hajishirzi_2024,
  title     = {Do Membership Inference Attacks Work on Large Language Models?},
  url       = {http://arxiv.org/abs/2402.07841},
  doi       = {10.48550/arXiv.2402.07841},
  note      = {arXiv:2402.07841 [cs]},
  booktitle = {Conference on Language Modeling (COLM)},
  publisher = {arXiv},
  author    = {Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
  year      = {2024},
  month     = sep
}

 @inproceedings{Carlini_Hayes_Nasr_Jagielski_Sehwag_Tramèr_Balle_Ippolito_Wallace_2023,
  address   = {Anaheim, CA, USA},
  title     = {Extracting Training Data from Diffusion Models},
  url       = {http://arxiv.org/abs/2301.13188},
  doi       = {10.48550/arXiv.2301.13188},
  note      = {arXiv:2301.13188 [cs]},
  booktitle = {Proceedings of the 32nd USENIX Security Symposium},
  publisher = {arXiv},
  author    = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  year      = {2023},
  month     = jan
}

 @article{Newton-Rex_2024,
  title        = {Suno is a music AI company aiming to generate \$120 billion per year. But is it trained on copyrighted recordings?},
  url          = {https://www.musicbusinessworldwide.com/suno-is-a-music-ai-company-aiming-to-generate-120-billion-per-year-newton-rex/},
  abstractnote = {Ed Newton-Rex discovers that Suno produces music with a striking resemblance to classic copyrights…},
  journal      = {Music Business Worldwide},
  author       = {Newton-Rex, Ed},
  year         = {2024},
  month        = apr,
  language     = {en-US}
}

 @article{Reed_2024,
  title        = {Does ChatGPT violate New York Times’ copyrights?},
  url          = {https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/},
  abstractnote = {Mason Kortz, a Harvard Law expert in technology and the law, says the New York Times lawsuit against ChatGPT parent OpenAI is the first big test for AI in the copyright space.},
  journal      = {Harvard Law School},
  author       = {Reed, Rachel},
  year         = {2024},
  month        = may,
  language     = {en-us}
}

@misc{copyrightlaw,
  title = {Copyright Law of the United States Title 17, Chapter 1, Section 107},
  url   = {https://www.copyright.gov/title17/92chap1.html#107},
  key   = {US-GOV}
}

 @article{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020,
  title     = {Scaling Laws for Neural Language Models},
  url       = {http://arxiv.org/abs/2001.08361},
  doi       = {10.48550/arXiv.2001.08361},
  note      = {arXiv:2001.08361 [cs]},
  number    = {arXiv:2001.08361},
  publisher = {arXiv},
  author    = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year      = {2020},
  month     = jan,
  journal   = {arXiv}
}

 @article{Marechal_2024,
  title        = {Generative AI: energy consumption soars},
  url          = {https://www.polytechnique-insights.com/en/columns/energy/generative-ai-energy-consumption-soars/},
  abstractnote = {Generative AI: energy consumption soars – Read the column on Polytechnique Insights},
  journal      = {Polytechnique Insights},
  author       = {Marechal, Anais},
  year         = {2024},
  month        = nov,
  language     = {en-GB}
}

 @inproceedings{Holzapfel_Kaila_Jääskeläinen_2024,
  address   = {San Francisco, United States},
  title     = {GREEN MIR? INVESTIGATING COMPUTATIONAL COST OF RECENT MUSIC-AI RESEARCH IN ISMIR},
  url       = {https://drive.google.com/file/d/1rAepoJk1U2R4g3S_AcxdqWDGzRGd7xPg/view?usp=embed_facebook},
  booktitle = {25th Int. Society for Music Information Retrieval Conf},
  author    = {Holzapfel, Andre and Kaila, Anna-Kaisa and Jääskeläinen, Petra},
  year      = {2024}
}

 @misc{Singh_2024,
  title        = {Indian filmmaker Ram Gopal Varma abandons human musicians for AI-generated music},
  url          = {https://techcrunch.com/2024/09/19/indian-filmmaker-ram-gopal-varma-abandons-human-musicians-for-ai-generated-music/},
  abstractnote = {Indian filmmaker Ram Gopal Varma is ditching human musicians for artificial intelligence, saying he’ll use only AI-generated tunes in future projects, a Indian filmmaker Ram Gopal Varma says he’ll use only AI-generated music in future projects.},
  journal      = {TechCrunch},
  author       = {Singh, Manish},
  year         = {2024},
  month        = sep,
  language     = {en-US}
}

 @inproceedings{Chen_Smith_Spijkervet_Wang_Zou_Li_Kong_Du_2024,
  address   = {San Fancisco, CA, USA},
  title     = {SymPAC: Scalable Symbolic Music Generation With Prompts And Constraints},
  url       = {http://arxiv.org/abs/2409.03055},
  doi       = {10.48550/arXiv.2409.03055},
  note      = {arXiv:2409.03055 [cs]},
  booktitle = {25th Int. Society for Music Information Retrieval Conference},
  author    = {Chen, Haonan and Smith, Jordan B. L. and Spijkervet, Janne and Wang, Ju-Chiang and Zou, Pei and Li, Bochen and Kong, Qiuqiang and Du, Xingjian},
  year      = {2024},
  month     = sep
}

 @article{Ji_Yang_Luo_survey_symbolic_2024,
  title    = {A Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges},
  volume   = {56},
  issn     = {0360-0300, 1557-7341},
  doi      = {10.1145/3597493},
  number   = {1},
  journal  = {ACM Computing Surveys},
  author   = {Ji, Shulei and Yang, Xinyu and Luo, Jing},
  year     = {2024},
  month    = jan,
  pages    = {1–39},
  language = {en}
}

 @inproceedings{Kingma_Welling_2014,
  title     = {Auto-Encoding Variational Bayes},
  url       = {http://arxiv.org/abs/1312.6114},
  doi       = {10.48550/arXiv.1312.6114},
  note      = {arXiv:1312.6114 [stat]},
  booktitle = {International Conference on Learning Representations (ICLR)},
  publisher = {arXiv},
  author    = {Kingma, Diederik P. and Welling, Max},
  year      = {2014}
}

 @article{Evans_Parker_Carr_Zukowski_Taylor_Pons_2024,
  title     = {Stable Audio Open},
  url       = {http://arxiv.org/abs/2407.14358},
  doi       = {10.48550/arXiv.2407.14358},
  note      = {arXiv:2407.14358 [cs]},
  number    = {arXiv:2407.14358},
  publisher = {arXiv},
  author    = {Evans, Zach and Parker, Julian D. and Carr, C. J. and Zukowski, Zack and Taylor, Josiah and Pons, Jordi},
  year      = {2024},
  month     = jul,
  journal   = {arXiv}
}

 @article{Van_Kranenburg_Volk_Wiering_2013,
  title    = {A Comparison between Global and Local Features for Computational Classification of Folk Song Melodies},
  volume   = {42},
  issn     = {0929-8215, 1744-5027},
  doi      = {10.1080/09298215.2012.718790},
  number   = {1},
  journal  = {Journal of New Music Research},
  author   = {Van Kranenburg, Peter and Volk, Anja and Wiering, Frans},
  year     = {2013},
  month    = mar,
  pages    = {1–18},
  language = {en}
}

 @article{Blacking_1971,
  title     = {Deep and Surface Structures in Venda Music},
  volume    = {3},
  issn      = {0316-6082},
  doi       = {10.2307/767458},
  journal   = {Yearbook of the International Folk Music Council},
  publisher = {International Council for Traditional Music},
  author    = {Blacking, John},
  year      = {1971},
  pages     = {91–108}
}

 @phdthesis{McKay_2004,
  address  = {Montreal QC Canada},
  title    = {Automatic Genre Classification Using Large High-Level Musical Feature Sets.},
  url      = {https://www.researchgate.net/publication/220723648_Automatic_Genre_Classification_Using_Large_High-Level_Musical_Feature_Sets},
  school   = {McGill},
  author   = {McKay, Cory},
  year     = {2004},
  language = {en}
}

 @article{Pinkerton_1956,
  title   = {INFORMATION THEORY AND MELODY},
  volume  = {194},
  number  = {2},
  journal = {Scientific American},
  author  = {Pinkerton, Richard C},
  year    = {1956},
  pages   = {77–87}
}

 @article{Rütte_figaro_2023,
  title   = {FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control},
  url     = {http://arxiv.org/abs/2201.10936},
  doi     = {10.48550/arXiv.2201.10936},
  note    = {arXiv:2201.10936},
  journal = {International Conference on Learning Representations},
  author  = {Rütte, Dimitri von and Biggio, Luca and Kilcher, Yannic and Hofmann, Thomas},
  year    = {2023}
}

 @inproceedings{Huang_Yang_remi_pop_transformer_2020,
  address    = {New York, NY, USA},
  series     = {MM ’20},
  title      = {Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions},
  isbn       = {978-1-4503-7988-5},
  url        = {https://doi.org/10.1145/3394171.3413671},
  doi        = {10.1145/3394171.3413671},
  booktitle  = {Proceedings of the 28th ACM International Conference on Multimedia},
  publisher  = {Association for Computing Machinery},
  author     = {Huang, Yu-Siang and Yang, Yi-Hsuan},
  year       = {2020},
  month      = oct,
  pages      = {1180–1188},
  collection = {MM ’20}
}

 @article{Lu_Xu_Kang_Yu_Xing_Tan_Bian_MuseCoco_2023,
  title     = {MuseCoco: Generating Symbolic Music from Text},
  url       = {http://arxiv.org/abs/2306.00110},
  doi       = {10.48550/arXiv.2306.00110},
  note      = {arXiv:2306.00110 [cs]},
  number    = {arXiv:2306.00110},
  publisher = {arXiv},
  journal   = {arXiv},
  author    = {Lu, Peiling and Xu, Xin and Kang, Chenfei and Yu, Botao and Xing, Chengyi and Tan, Xu and Bian, Jiang},
  year      = {2023},
  month     = may
}

 @article{Wu_Donahue_musicontrolnet_2023,
  title     = {Music ControlNet: Multiple Time-varying Controls for Music Generation},
  url       = {http://arxiv.org/abs/2311.07069},
  note      = {arXiv:2311.07069 [cs]},
  number    = {arXiv:2311.07069},
  publisher = {arXiv},
  author    = {Wu, Shih-Lun and Donahue, Chris and Watanabe, Shinji and Bryan, Nicholas J.},
  year      = {2023},
  month     = nov,
  language  = {en},
  journal   = {arXiv}
}

 @inproceedings{Lan_Hsiao_Cheng_Yang_musicongen_2024,
  address   = {San Fancisco, CA, USA},
  title     = {MusiConGen: Rhythm and Chord Control for Transformer-Based Text-to-Music Generation},
  url       = {http://arxiv.org/abs/2407.15060},
  note      = {arXiv:2407.15060 [cs]},
  booktitle = {25th Int. Society for Music Information Retrieval Conference},
  publisher = {arXiv},
  author    = {Lan, Yun-Han and Hsiao, Wen-Yi and Cheng, Hao-Chung and Yang, Yi-Hsuan},
  year      = {2024},
  month     = jul,
  language  = {en}
}

@inproceedings{Rouard_Adi_Copet_Roebel_Défossez_musicgenstyle_2024,
  address   = {San Fancisco, CA, USA},
  title     = {Audio Conditioning for Music Generation via Discrete Bottleneck Features},
  url       = {http://arxiv.org/abs/2407.12563},
  doi       = {10.48550/arXiv.2407.12563},
  note      = {arXiv:2407.12563},
  booktitle = {25th Int. Society for Music Information Retrieval Conference},
  publisher = {arXiv},
  author    = {Rouard, Simon and Adi, Yossi and Copet, Jade and Roebel, Axel and Défossez, Alexandre},
  year      = {2024},
  month     = jul
}


 @inproceedings{Min_Jiang_Xia_Zhao_polyffusion_2023,
  address   = {Milan, Italy},
  title     = {Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls},
  url       = {http://arxiv.org/abs/2307.10304},
  note      = {arXiv:2307.10304 [cs]},
  booktitle = {24th Int. Society for Music Information Retrieval Conference (ISMIR 2023)},
  publisher = {arXiv},
  author    = {Min, Lejun and Jiang, Junyan and Xia, Gus and Zhao, Jingwei},
  year      = {2023},
  month     = jul
}

 @article{Yin_Reuben_Stepney_Collins_2023,
  title    = {Deep learning’s shallow gains: a comparative evaluation of algorithms for automatic music generation},
  volume   = {112},
  issn     = {1573-0565},
  doi      = {10.1007/s10994-023-06309-w},
  number   = {5},
  journal  = {Machine Learning},
  author   = {Yin, Zongyu and Reuben, Federico and Stepney, Susan and Collins, Tom},
  year     = {2023},
  month    = may,
  pages    = {1785–1822},
  language = {en}
}

 @article{Collins_Laney_2017,
  title     = {Computer-Generated Stylistic Compositions with Long-Term Repetitive and Phrasal Structure},
  volume    = {1},
  issn      = {2399-7656},
  url       = {https://www.jcms.org.uk/article/id/510/},
  doi       = {10.5920/JCMS.2017.02},
  number    = {22},
  journal   = {Journal of Creative Music Systems},
  publisher = {Huddersfield University Press},
  author    = {Collins, Tom and Laney, Robin},
  year      = {2017},
  month     = mar,
  language  = {None}
}

 @article{Huang_Vaswani_Uszkoreit_Shazeer_Simon_Hawthorne_Dai_Hoffman_Dinculescu_Eck_2018,
  title     = {Music Transformer},
  url       = {http://arxiv.org/abs/1809.04281},
  doi       = {10.48550/arXiv.1809.04281},
  note      = {arXiv:1809.04281},
  number    = {arXiv:1809.04281},
  publisher = {arXiv},
  author    = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
  journal   = {arXiv},
  year      = {2018},
  month     = dec
}

 @article{Funk_2018,
  title     = {A Musical Suite Composed by an Electronic Brain: Reexamining the Illiac Suite <em>and the Legacy of Lejaren A. Hiller Jr.</em>},
  volume    = {28},
  issn      = {0961-1215},
  journal   = {Leonardo Music Journal},
  publisher = {The MIT Press},
  author    = {Funk, Tiffany},
  year      = {2018},
  pages     = {19–24}
}

 @book{Fux_1725,
  address   = {Vienna, Austria},
  title     = {Gradus ad Parnassum},
  url       = {https://www.loc.gov/item/16002789/},
  publisher = {Catholicae Majestati Aulae-Typographi},
  author    = {Fux, Johann Joseph},
  year      = {1725}
}

 @article{Mirelman_2013,
  title    = {Tuning procedures in ancient Iraq},
  number   = {2},
  journal  = {Analytical Approaches to World Music 2},
  author   = {Mirelman, Sam},
  year     = {2013},
  pages    = {43–56},
  language = {en}
}

 @article{Ebcioğlu_1994,
  title   = {An expert system for harmonizing chorales in the style of J. S. Bach},
  volume  = {eds. M. Balaban, K. Ebcioğlu, and O. Laske},
  journal = {Understanding Music with AI: Perspectives on Music Cognition},
  author  = {Ebcioğlu, Kamal},
  year    = {1994},
  pages   = {145–185}
}

 @phdthesis{Allan_2002,
  title        = {Harmonising Chorales in the Style of Johann Sebastian Bach},
  url          = {https://www.researchgate.net/publication/238242337_Harmonising_Chorales_in_the_Style_of_Johann_Sebastian_Bach},
  abstractnote = {PDF | Abstract This dissertation describes a chorale harmonisation system which uses Hidden Markov Models. We use a standard data set of chorale... | Find, read and cite all the research you need on ResearchGate},
  school       = {Edinborough},
  author       = {Allan, Moray},
  year         = {2002},
  language     = {en}
}

 @article{Pachet_2003,
  title        = {The Continuator: Musical Interaction With Style},
  volume       = {32},
  issn         = {0929-8215},
  doi          = {10.1076/jnmr.32.3.333.16861},
  abstractnote = {We propose a system, the Continuator, that bridges the gap between two classes of traditionally incompatible musical systems: (1) interactive musical systems, limited in their ability to generate stylistically consistent material, and (2) music imitation systems, which are fundamentally not interactive. Our purpose is to allow musicians to extend their technical ability with stylistically consistent, automatically learnt material. This goal requires the ability for the system to build operational representations of musical styles in a real time context. Our approach is based on a Markov model of musical styles augmented to account for musical issues such as management of rhythm, beat, harmony, and imprecision. The resulting system is able to learn and generate music in any style, either in standalone mode, as continuations of musician’s input, or as interactive improvisation back up. Lastly, the very design of the system makes possible new modes of musical collaborative playing. We describe the architecture, implementation issues and experimentations conducted with the system in several real world contexts.},
  number       = {3},
  journal      = {Journal of New Music Research},
  publisher    = {Routledge},
  author       = {Pachet, François},
  year         = {2003},
  month        = sep,
  pages        = {333–341}
}

 @inproceedings{Altay_Alatas_2018,
  title     = {Music based metaheuristic methods for constrained optimization},
  url       = {https://ieeexplore.ieee.org/abstract/document/8355355},
  doi       = {10.1109/ISDFS.2018.8355355},
  booktitle = {2018 6th International Symposium on Digital Forensic and Security (ISDFS 2018)},
  author    = {Altay, Elif Varol and Alatas, Bilal},
  year      = {2018},
  month     = mar,
  pages     = {1–6}
}

 @article{heuristic_harmony_Geem_2001,
  title        = {A new heuristic optimization algorithm: Harmony search},
  volume       = {76},
  doi          = {10.1177/003754970107600201},
  abstractnote = {Many optimization problems in various fields have been solved using diverse optimization al gorithms. Traditional optimization techniques such as linear programming (LP), non-linear programming (NLP), and dynamic program ming (DP) have had major roles in solving these problems. However, their drawbacks generate demand for other types of algorithms, such as heuristic optimization approaches (simulated annealing, tabu search, and evolutionary algo rithms). However, there are still some possibili ties of devising new heuristic algorithms based on analogies with natural or artificial phenom ena. A new heuristic algorithm, mimicking the improvisation of music players, has been devel oped and named Harmony Search (HS). The performance of the algorithm is illustrated with a traveling salesman problem (TSP), a specific academic optimization problem, and a least-cost pipe network design problem.},
  note         = {Citation Key: doi:10.1177/003754970107600201
                  tex.eprint: https://doi.org/10.1177/003754970107600201},
  number       = {2},
  journal      = {SIMULATION},
  author       = {Geem, Zong Woo and Kim, Joong Hoon and Loganathan, G.V.},
  year         = {2001},
  pages        = {60–68}
}


 @article{Polito_Daida_Bersano-Begey_1997,
  series       = {Lecture Notes in Computer Science},
  title        = {Musica ex machina: Composing 16th-century counterpoint with genetic programming and symbiosis},
  volume       = {1213},
  doi          = {10.1007/BFb0014805},
  abstractnote = {GPmuse is software which explores one connection between computation and creativity using a symbiosis-inspired genetic programming paradigm in which distinct agents collaborate to produce 16th-century counterpoint.},
  note         = {Book Title: Evolutionary Programming VI},
  journal      = {Evolutionary Programming},
  author       = {Polito, John and Daida, Jason M. and Bersano-Begey, Tommaso F.},
  editor       = {Angeline, Peter J. and Reynolds, Robert G. and McDonnell, John R. and Eberhart, Russ},
  year         = {1997},
  pages        = {113–123},
  collection   = {Lecture Notes in Computer Science}
}

 @article{Hild_Feulner_Menzel_1991,
  title    = {HARMONET: A Neural Net for Harmonizing Chorales in the Style of l.S.Bach},
  journal  = {Advances in Neural Information Processing Systems 4 (NIPS 1991)},
  author   = {Hild, Hermann and Feulner, Johannes and Menzel, Wolfram},
  year     = {1991},
  language = {en}
}

 @article{Civit_Civit-Masot_Cuadrado_Escalona_2022,
  title   = {A systematic review of artificial intelligence-based music generation: Scope, applications, and future trends},
  volume  = {209},
  issn    = {0957-4174},
  doi     = {10.1016/j.eswa.2022.118190},
  journal = {Expert Systems with Applications},
  author  = {Civit, Miguel and Civit-Masot, Javier and Cuadrado, Francisco and Escalona, Maria J.},
  year    = {2022},
  month   = dec,
  pages   = {118190}
}

 @article{Todd_1989,
  title     = {A Connectionist Approach to Algorithmic Composition},
  volume    = {13},
  issn      = {0148-9267},
  doi       = {10.2307/3679551},
  number    = {4},
  journal   = {Computer Music Journal},
  publisher = {The MIT Press},
  author    = {Todd, Peter M.},
  year      = {1989},
  pages     = {27–43}
}

 @article{Mozer_1994,
  title     = {Neural Network Music Composition by Prediction: Exploring the Benefits of Psychoacoustic Constraints and Multi-scale Processing},
  volume    = {6},
  issn      = {0954-0091},
  doi       = {10.1080/09540099408915726},
  number    = {2–3},
  journal   = {Connection Science},
  publisher = {Taylor \& Francis},
  author    = {Mozer, Michael C.},
  year      = {1994},
  month     = jan,
  pages     = {247–280}
}

 @article{Sutskever_Vinyals_Le_2014,
  title    = {Sequence to Sequence Learning with Neural Networks},
  url      = {http://arxiv.org/abs/1409.3215},
  doi      = {10.48550/arXiv.1409.3215},
  note     = {arXiv:1409.3215 [cs]},
  journal  = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
  author   = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year     = {2014},
  month    = dec,
  language = {en}
}

 @article{Défossez_2023_encodec,
  title    = {High Fidelity Neural Audio Compression},
  url      = {http://arxiv.org/abs/2210.13438},
  doi      = {10.48550/arXiv.2210.13438},
  note     = {arXiv:2210.13438 [eess]},
  journal  = {Transactions on Machine Learning Research},
  author   = {Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
  year     = {2023},
  language = {en}
}

 @article{Fradet_Briot_Chhel_2021,
  title    = {MIDITOK: A PYTHON PACKAGE FOR MIDI FILE TOKENIZATION},
  journal  = {Extended Abstracts for the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference (ISMIR 2021)},
  author   = {Fradet, Nathan and Briot, Jean-Pierre and Chhel, Fabien},
  year     = {2021},
  language = {en}
}

 @inproceedings{Chen_Wu_MusicLDM_2023,
  title     = {MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies},
  url       = {http://arxiv.org/abs/2308.01546},
  doi       = {10.48550/arXiv.2308.01546},
  note      = {arXiv:2308.01546 [cs]},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024)},
  publisher = {arXiv},
  author    = {Chen, Ke and Wu, Yusong and Liu, Haohe and Nezhurina, Marianna and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
  year      = {2023},
  month     = aug
}

 @inproceedings{Zhang_Rao_Agrawala_2023,
  title     = {Adding Conditional Control to Text-to-Image Diffusion Models},
  url       = {http://arxiv.org/abs/2302.05543},
  doi       = {10.1109/ICCV51070.2023.00355},
  note      = {arXiv:2302.05543 [cs]},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV 2023)},
  publisher = {arXiv},
  author    = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  year      = {2023},
  month     = nov
}

 @inproceedings{Liu_Chen_Yuan_Mei_Liu_Mandic_Wang_Plumbley_2023,
  title     = {AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},
  url       = {http://arxiv.org/abs/2301.12503},
  doi       = {10.48550/arXiv.2301.12503},
  note      = {arXiv:2301.12503 [cs]},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (PMLR)},
  publisher = {arXiv},
  author    = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
  year      = {2023},
  month     = sep
}

 @article{Lin_cocomulla_2024,
  title     = {Content-based Controls For Music Large Language Modeling},
  url       = {http://arxiv.org/abs/2310.17162},
  doi       = {10.48550/arXiv.2310.17162},
  note      = {arXiv:2310.17162 [cs]},
  number    = {arXiv:2310.17162},
  publisher = {arXiv},
  author    = {Lin, Liwei and Xia, Gus and Jiang, Junyan and Zhang, Yixiao},
  year      = {2024},
  month     = oct,
  language  = {en},
  journal   = {arXiv}
}

 @article{Koo_Wichern_Germain_SMITIN_2024,
  title   = {SMITIN: Self-Monitored Inference-Time INtervention for Generative Music Transformers},
  url     = {http://arxiv.org/abs/2404.02252},
  doi     = {10.48550/arXiv.2404.02252},
  note    = {arXiv:2404.02252},
  journal = {IEEE Open Journal of Signal Processing 2025},
  author  = {Koo, Junghyun and Wichern, Gordon and Germain, Francois G. and Khurana, Sameer and Roux, Jonathan Le},
  year    = {2024},
  month   = apr
}

 @inproceedings{Dong_Chen_MMT_Kirkpatrick_2023,
  title     = {Multitrack Music Transformer},
  url       = {http://arxiv.org/abs/2207.06983},
  doi       = {10.48550/arXiv.2207.06983},
  note      = {arXiv:2207.06983},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)},
  publisher = {arXiv},
  author    = {Dong, Hao-Wen and Chen, Ke and Dubnov, Shlomo and McAuley, Julian and Berg-Kirkpatrick, Taylor},
  year      = {2023},
  month     = may
}

 @article{Zeng_Tan_Wang_MUSICBERT_2021,
  title    = {MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training},
  doi      = {10.18653/v1/2021.findings-acl.70},
  note     = {arXiv:2106.05630 [cs]},
  journal  = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  author   = {Zeng, Mingliang and Tan, Xu and Wang, Rui and Ju, Zeqian and Qin, Tao and Liu, Tie-Yan},
  year     = {2021},
  month    = jun,
  pages    = {791–800},
  language = {en}
}
 @inproceedings{Rombach_Blattmann_Lorenz_Esser_Ommer_2022,
  title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  year      = {2022},
  pages     = {10684–10695},
  language  = {en}
}


 @article{Lee_Doh_Jeong_2023_subjectivity_musiccaps,
  title    = {Annotator Subjectivity in the MusicCaps Dataset},
  volume   = {1},
  journal  = {HCMIR23: 2nd Workshop on Human-Centric Music Information Research)},
  author   = {Lee, Minhee and Doh, SeungHeon and Jeong, Dasaem},
  year     = {2023},
  month    = nov,
  language = {en}
}

 @article{Xiong_Wang_ai_eval_methods_2023,
  title     = {A Comprehensive Survey for Evaluation Methodologies of AI-Generated Music},
  url       = {http://arxiv.org/abs/2308.13736},
  doi       = {10.48550/arXiv.2308.13736},
  note      = {arXiv:2308.13736 [cs]},
  number    = {arXiv:2308.13736},
  publisher = {arXiv},
  author    = {Xiong, Zeyu and Wang, Weitao and Yu, Jing and Lin, Yue and Wang, Ziyan},
  journal   = {arXiv},
  year      = {2023},
  month     = aug
}

 @article{Gurjar_Moon_similarity_2018,
  title    = {A Comparative Analysis of Music Similarity Measures in Music Information Retrieval Systems},
  volume   = {14},
  doi      = {10.3745/JIPS.04.0054},
  number   = {1},
  journal  = {Journal of Information Processing Systems},
  author   = {Gurjar, Kuldeep and Moon, Yang-Sae},
  year     = {2018},
  month    = feb,
  pages    = {32–55},
  language = {en}
}

 @inbook{Volk_Garbers_VanKranenburg_Wiering_Grijp_Veltkamp_2009,
  address    = {Berlin, Heidelberg},
  series     = {Communications in Computer and Information Science},
  title      = {Comparing Computational Approaches to Rhythmic and Melodic Similarity in Folksong Research},
  volume     = {37},
  isbn       = {978-3-642-04578-3},
  url        = {http://link.springer.com/10.1007/978-3-642-04579-0_8},
  doi        = {10.1007/978-3-642-04579-0_8},
  booktitle  = {Mathematics and Computation in Music},
  publisher  = {Springer Berlin Heidelberg},
  author     = {Volk, Anja and Garbers, Jörg and Van Kranenburg, Peter and Wiering, Frans and Grijp, Louis and Veltkamp, Remco C.},
  editor     = {Klouche, Timour and Noll, Thomas},
  year       = {2009},
  pages      = {78–87},
  collection = {Communications in Computer and Information Science},
  language   = {en}
}

 @article{Kilgour_Frachet_2019,
  title    = {Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms},
  url      = {http://arxiv.org/abs/1812.08466},
  doi      = {10.48550/arXiv.1812.08466},
  note     = {arXiv:1812.08466 [eess]},
  journal  = {INTERSPEECH 2019},
  author   = {Kilgour, Kevin and Zuluaga, Mauricio and Roblek, Dominik and Sharifi, Matthew},
  year     = {2019},
  month    = jan,
  language = {en}
}

 @inproceedings{Elizalde_Deshmukh_Ismail_Wang_2023,
  title     = {CLAP: Learning Audio Concepts From Natural Language Supervision},
  url       = {http://arxiv.org/abs/2206.04769},
  doi       = {10.48550/arXiv.2206.04769},
  note      = {arXiv:2206.04769 [cs]},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)},
  publisher = {arXiv},
  author    = {Elizalde, Benjamin and Deshmukh, Soham and Ismail, Mahmoud Al and Wang, Huaming},
  year      = {2023},
  language  = {en}
}

 @phdthesis{Raffel_2016,
  address  = {New York, NY, USA},
  title    = {Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching},
  school   = {Columbia University},
  author   = {Raffel, Colin},
  year     = {2016},
  language = {en}
}

 @inproceedings{Radford_Wu_Child_Luan_gpt2_2019,
  title     = {Language models are unsupervised multitask learners},
  url       = {https://api.semanticscholar.org/CorpusID:160025533},
  author    = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year      = {2019},
  booktitle = {}
}

 @inproceedings{Tal_jasco,
  address   = {San Fancisco, CA, USA},
  title     = {Joint Audio and Symbolic Conditioning for Temporally Controlled Text-to-Music Generation},
  url       = {http://arxiv.org/abs/2406.10970},
  note      = {arXiv:2406.10970 [cs]},
  booktitle = {25th Int. Society for Music Information Retrieval Conference},
  author    = {Tal, Or and Ziv, Alon and Gat, Itai and Kreuk, Felix and Adi, Yossi},
  year      = {2024},
  month     = jun,
  language  = {en}
}

 @article{Zhu_Liu_Jiang_Zheng_texture_2024,
  title     = {Symbolic Music Generation with Fine-grained Interactive Textural Guidance},
  url       = {http://arxiv.org/abs/2410.08435},
  doi       = {10.48550/arXiv.2410.08435},
  note      = {arXiv:2410.08435 [cs]},
  number    = {arXiv:2410.08435},
  publisher = {arXiv},
  author    = {Zhu, Tingyu and Liu, Haoyu and Jiang, Zhimin and Zheng, Zeyu},
  year      = {2024},
  month     = oct,
  language  = {en},
  journal   = {arXiv}
}

 @inproceedings{Huang_rule_diffusion_2024,
  address   = {Vienna, Austria},
  title     = {Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion},
  url       = {http://arxiv.org/abs/2402.14285},
  doi       = {10.48550/arXiv.2402.14285},
  note      = {arXiv:2402.14285 [cs]},
  booktitle = {The Forty-First International Conference on Machine Learning},
  publisher = {arXiv},
  author    = {Huang, Yujia and Ghatare, Adishree and Liu, Yuanzhe and Hu, Ziniu and Zhang, Qinsheng and Sastry, Chandramouli S. and Gururani, Siddharth and Oore, Sageev and Yue, Yisong},
  year      = {2024},
  month     = sep
}


 @inproceedings{Wang_vae_chord_rhythm_2020,
  address   = {Montreal QC Canada},
  title     = {Learning Interpretable Representation for Controllable Polyphonic Music Generation},
  url       = {http://arxiv.org/abs/2008.07122},
  doi       = {10.48550/arXiv.2008.07122},
  note      = {arXiv:2008.07122 [cs]},
  booktitle = {21st International Conference on Music Information Retrieval (ISMIR 2020)},
  publisher = {arXiv},
  author    = {Wang, Ziyu and Wang, Dingsu and Zhang, Yixiao and Xia, Gus},
  year      = {2020},
  month     = aug
}

 @inproceedings{Ryu_Dong_nested_2024,
  title     = {Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation},
  url       = {http://arxiv.org/abs/2408.01180},
  doi       = {10.48550/arXiv.2408.01180},
  note      = {arXiv:2408.01180 [cs]},
  booktitle = {25th Int. Society for Music Information Retrieval Conference (ISMIR 2024)},
  author    = {Ryu, Jiwoo and Dong, Hao-Wen and Jung, Jongmin and Jeong, Dasaem},
  year      = {2024},
  month     = aug
}

 @inproceedings{Sennrich_Haddow_Birch_BPE_2016,
  address   = {Berlin, Germany},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  url       = {http://arxiv.org/abs/1508.07909},
  doi       = {10.48550/arXiv.1508.07909},
  note      = {arXiv:1508.07909 [cs]},
  booktitle = {Association for Computational Linguistics (ACL 2016)},
  publisher = {arXiv},
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year      = {2016},
  month     = jun
}

 @inproceedings{Chew_Volk_Lee_Dance_metric_weight_2005,
  address   = {Boston, MA},
  title     = {Dance Music Classification Using Inner Metric Analysis},
  isbn      = {978-0-387-23529-5},
  doi       = {10.1007/0-387-23529-9_23},
  booktitle = {The Next Wave in Computing, Optimization, and Decision Technologies},
  publisher = {Springer US},
  author    = {Chew, Elaine and Volk, Anja and Lee, Chia-Ying},
  editor    = {Golden, Bruce and Raghavan, S. and Wasil, Edward},
  year      = {2005},
  pages     = {355–370},
  language  = {en}
}

 @inproceedings{Haas_Volk_2016,
  title     = {Meter Detection in Symbolic Music Using Inner Metric Analysis},
  booktitle = {International Society for Music Information Retrieval Conference (ISMIR 2016)},
  url       = {https://www.semanticscholar.org/paper/Meter-Detection-in-Symbolic-Music-Using-Inner-Haas-Volk/8a72fc0874e0476f3dc0b92a109ebf854a4551cc},
  author    = {Haas, W. B. and Volk, A.},
  year      = {2016},
  month     = aug
}

 @article{Shu_Xu_Musebarcontrol_2024,
  title     = {MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss},
  url       = {http://arxiv.org/abs/2407.04331},
  doi       = {10.48550/arXiv.2407.04331},
  note      = {arXiv:2407.04331 [cs]},
  number    = {arXiv:2407.04331},
  publisher = {arXiv},
  author    = {Shu, Yangyang and Xu, Haiming and Zhou, Ziqin and Hengel, Anton van den and Liu, Lingqiao},
  year      = {2024},
  month     = jul,
  journal   = {arXiv}
}

 @article{Liebel_Körner_2018_auxiliary_task,
  title     = {Auxiliary Tasks in Multi-task Learning},
  url       = {http://arxiv.org/abs/1805.06334},
  doi       = {10.48550/arXiv.1805.06334},
  journal   = {arXiv},
  note      = {arXiv:1805.06334 [cs]},
  number    = {arXiv:1805.06334},
  publisher = {arXiv},
  author    = {Liebel, Lukas and Körner, Marco},
  year      = {2018},
  month     = may
}

@article{Ens_Pasquier_2020_MMM,
  title     = {MMM: Exploring Conditional Multi-Track Music Generation with the Transformer},
  url       = {http://arxiv.org/abs/2008.06048},
  doi       = {10.48550/arXiv.2008.06048},
  note      = {arXiv:2008.06048 [cs]},
  number    = {arXiv:2008.06048},
  publisher = {arXiv},
  author    = {Ens, Jeff and Pasquier, Philippe},
  year      = {2020},
  month     = aug,
  journal   = {arXiv}
}

 @inproceedings{Yu_Lu_Wang_Hu_Tan_Ye_Zhang_museformer_2022,
  title     = {Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation},
  url       = {http://arxiv.org/abs/2210.10349},
  doi       = {10.48550/arXiv.2210.10349},
  note      = {arXiv:2210.10349 [cs]},
  booktitle = {Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)},
  publisher = {arXiv},
  author    = {Yu, Botao and Lu, Peiling and Wang, Rui and Hu, Wei and Tan, Xu and Ye, Wei and Zhang, Shikun and Qin, Tao and Liu, Tie-Yan},
  year      = {2022},
  month     = oct,
  language  = {en}
}

 @inproceedings{Tan_Herremans_2020,
  address      = {Montreal QC Canada},
  title        = {Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling},
  url          = {https://www.researchgate.net/publication/343500818_Music_FaderNets_Controllable_Music_Generation_Based_On_High-Level_Features_via_Low-Level_Feature_Modelling},
  abstractnote = {High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to... | Find, read and cite all the research you need on ResearchGate},
  booktitle    = {Proc. of 21st International Society of Music Information Retrieval Conference, ISMIR 2020},
  author       = {Tan, Hao and Herremans, Dorien},
  year         = {2020},
  language     = {en}
}

@inproceedings{hawthorne2018maestro,
  title     = {Enabling Factorized Piano Music Modeling and Generation with the {MAESTRO} Dataset},
  author    = {Curtis Hawthorne and Andriy Stasyuk and Adam Roberts and Ian Simon and Cheng-Zhi Anna Huang and Sander Dieleman and Erich Elsen and Jesse Engel and Douglas Eck},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=r1lYRjC9F7}
}
@misc{jsbchorales,
  key   = {jsbchorals},
  title = {Canonical JSB Dataset Chorals 389},
  url   = {https://github.com/czhuang/JSB-Chorales-dataset}
}

@misc{sessionfolkdata,
  author = {Jeremy Keith},
  title  = {TheSession Data},
  url    = {https://github.com/adactio/TheSession-data}
}

@misc{hooktheorypopmidi,
  title = {Hook Theory collection of pop-song transcriptions},
  url   = {https://www.hooktheory.com/theorytab},
  key   = {hooktheory}
}

@misc{bitmidi,
  title = {Bit Midi Dataset Donation},
  url   = {https://bitmidi.com/},
  key   = {bitmidi}
}


@misc{classicalarchives,
  title = {Classical Archives Dataset Donation},
  url   = {https://www.classicalarchives.com/newca/#!/},
  key   = {cadd}
}
 @inproceedings{Wang_Chen_pop90_dataset,
  title     = {POP909: A POP-SONG DATASET FOR MUSIC ARRANGEMENT GENERATION},
  booktitle = {21st International Conference on Music Information Retrieval (ISMIR 2020)},
  author    = {Wang, Ziyu and Chen, Ke and Jiang, Junyan and Zhang, Yiyi and Xu, Maoran and Dai, Shuqi and Gu, Xianbin and Xia, Gus},
  language  = {en},
  year      = {2020}
}

 @inproceedings{Crestel_OrchestralDataset,
  address   = {Suzhou, China},
  title     = {A database linking piano and orchestral MIDI scores with application to automatic projective orchestration},
  url       = {http://arxiv.org/abs/1810.08611},
  doi       = {10.48550/arXiv.1810.08611},
  note      = {arXiv:1810.08611 [cs]},
  booktitle = {18th International Society for Music Information Retrieval Conference},
  publisher = {arXiv},
  author    = {Crestel, Léopold and Esling, Philippe and Heng, Lena and McAdams, Stephen},
  year      = {2018},
  month     = oct
}

 @inproceedings{Bertin-Mahieux_Ellis_Whitman_Lamere_2011,
  title     = {The million song dataset},
  booktitle = {Proceedings of the 12th international conference on music information retrieval (ISMIR 2011)},
  author    = {Bertin-Mahieux, Thierry and Ellis, Daniel P.W. and Whitman, Brian and Lamere, Paul},
  year      = {2011}
}

 @inproceedings{Devlin_Chang_Lee_ToutanovaBERT_2019,
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url          = {http://arxiv.org/abs/1810.04805},
  doi          = {10.48550/arXiv.1810.04805},
  abstractnote = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
  note         = {arXiv:1810.04805 [cs]},
  booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies (ACL 2019)},
  publisher    = {arXiv},
  author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year         = {2019},
  month        = may,
  language     = {en}
}

 @book{Nierhaus_2009,
  address   = {Vienna},
  title     = {Algorithmic Composition (pp 36\&38)},
  rights    = {http://www.springer.com/tdm},
  isbn      = {978-3-211-75539-6},
  url       = {http://link.springer.com/10.1007/978-3-211-75540-2},
  doi       = {10.1007/978-3-211-75540-2},
  publisher = {Springer},
  author    = {Nierhaus, Gerhard},
  year      = {2009},
  language  = {en}
}

@misc{Kite-Powell_2023,
  title        = {See How This AI Improvised With A Musican To Create A Musical Dialogue In Real-Time},
  url          = {https://www.forbes.com/sites/jenniferhicks/2023/02/25/see-how-this-ai-improvised-with-a-musican-to-create-a-musical-dialogue-in-real-time/},
  abstractnote = {A 75-minute live performance at the Guildhall School of Music in London featured an improvised performance between a human and an AI that resulted in a tonal musical dialogue in real-time.},
  journal      = {Forbes},
  author       = {Kite-Powell, Jennifer},
  year         = {2023},
  month        = feb,
  language     = {en}
}

@incollection{Djaouti2011,
  author    = {Djaouti, Damien and Alvarez, Julian and Jessel, Jean-Pierre},
  title     = {Classifying Serious Games: The G/P/S Model},
  booktitle = {Handbook of Research on Improving Learning and Motivation through Educational Games: Multidisciplinary Approaches},
  editor    = {Felicia, Patrick},
  pages     = {118--136},
  year      = {2011},
  publisher = {IGI Global},
  doi       = {10.4018/978-1-60960-495-0.ch006},
  url       = {https://www.igi-global.com/chapter/classifying-serious-games/52492}
}

@mastersthesis{Schlette_2022,
  title        = {Increasing engagement in a MACT game through feedback and adaptability},
  abstractnote = {This thesis investigates improving attention control for Parkinson Disease patients in the context of Musical Attention Control Training (MACT). This is done by implementing a form of performance feedback within a pre-existing serious game. By applying concepts from providing feedback in serious games and introducing higher levels of musical diversity. More specifically applying Dynamic Difficulty Adjustment(DDA) using rhythmic difficulty levels. These levels are subdivided by difficulty based upon measures of syncopation. An experiment was conducted with 14 participants testing the proposed improvements. The data being gathered consisted of game-scores and game experience using a pre-and post game survey. These results were extended using qualitative interviews with participants and a music therapist. Different approaches for providing feedback and enhancing the gaming experience are discussed. Recommendations are given for future implementations and practical limitations are acknowledged. Results indicate added value for providing feedback, although more focus can be on the personalisation of the provided messages. Further suggestions are provided on how to improve engagement within a MACT-game.},
  school       = {Utrecht University},
  author       = {Schlette, Luuk},
  year         = {2022},
  language     = {en}
}

 @article{coconet,
  title        = {CoCoNet: A Collaborative Convolutional Network},
  url          = {http://arxiv.org/abs/1901.09886},
  doi          = {10.48550/arXiv.1901.09886},
  abstractnote = {We present an end-to-end deep network for fine-grained visual categorization called Collaborative Convolutional Network (CoCoNet). The network uses a collaborative layer after the convolutional layers to represent an image as an optimal weighted collaboration of features learned from training samples as a whole rather than one at a time. This gives CoCoNet more power to encode the fine-grained nature of the data with limited samples. We perform a detailed study of the performance with 1-stage and 2-stage transfer learning. The ablation study shows that the proposed method outperforms its constituent parts consistently. CoCoNet also outperforms few state-of-the-art competing methods. Experiments have been performed on the fine-grained bird species classification problem as a representative example, but the method may be applied to other similar tasks. We also introduce a new public dataset for fine-grained species recognition, that of Indian endemic birds and have reported initial results on it.},
  note         = {arXiv:1901.09886 [cs]},
  number       = {arXiv:1901.09886},
  publisher    = {arXiv},
  author       = {Chakraborti, Tapabrata and McCane, Brendan and Mills, Steven and Pal, Umapada},
  year         = {2020},
  month        = nov
}

 @article{Herremans_Chew_Morpheus_2019,
  title        = {MorpheuS: generating structured music with constrained patterns and tension},
  volume       = {10},
  issn         = {1949-3045, 2371-9850},
  doi          = {10.1109/TAFFC.2017.2737984},
  abstractnote = {Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS’ novel framework has the ability to generate polyphonic pieces with a given tension proﬁle and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantiﬁes the tension proﬁle and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efﬁcient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best ﬁt the prescribed tension proﬁle to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with speciﬁc tension proﬁle and long-term structure is particularly useful in a game or ﬁlm music context. Music generated by the MorpheuS system has been performed live in concerts.},
  note         = {arXiv:1812.04832 [cs]},
  number       = {4},
  journal      = {IEEE Transactions on Affective Computing},
  author       = {Herremans, Dorien and Chew, Elaine},
  year         = {2019},
  month        = oct,
  pages        = {510–523},
  language     = {en}
}

 @article{Collins_Laney_2017,
  title        = {Computer-Generated Stylistic Compositions with Long-Term Repetitive and Phrasal Structure},
  volume       = {1},
  issn         = {2399-7656},
  url          = {https://www.jcms.org.uk/article/id/510/},
  doi          = {10.5920/JCMS.2017.02},
  abstractnote = {This article describes and evaluates an algorithm called Racchmaninof-Jun2015, referred to hereafter as Racchmaninof, which generates passages of music in a specifiable style. For generating all four parts of a Bach hymn (one of two target styles evaluated as part of a listening study – the other being Chopin mazurkas), we found that only five out of 25 participants performed significantly better than chance at distinguishing Racchmaninof’s output from original human compositions. These participants had a mean of 8.56 years of formal musical training and mode “daily/weekly” regularity of playing an instrument or singing. In the context of relatively high levels of musical expertise, this difficulty of distinguishing Racchmaninof’s output from original human compositions underlines the promise of our approach. Current trends and issues in the area of automatic stylistic composition are introduced and discussed and we consider the potential for applying our algorithm to additional composers and/or genres of music.},
  number       = {22},
  journal      = {Journal of Creative Music Systems},
  publisher    = {Huddersfield University Press},
  author       = {Collins, Tom and Laney, Robin},
  year         = {2017},
  month        = mar,
  language     = {None}
}

 @inproceedings{compound_word_Hsiao_Liu_Yeh_Yang_2021,
  title        = {Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs},
  url          = {http://arxiv.org/abs/2101.02402},
  doi          = {10.48550/arXiv.2101.02402},
  abstractnote = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note’s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5--10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.},
  note         = {arXiv:2101.02402 [cs]},
  booktitle    = {AAAI Conference on Artificial Intelligence},
  publisher    = {arXiv},
  author       = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
  year         = {2021},
  month        = jan
}

 @inproceedings{Fradet_Gutowski_Chhel_Briot_2023,
  title        = {Byte Pair Encoding for Symbolic Music},
  url          = {http://arxiv.org/abs/2301.11975},
  doi          = {10.48550/arXiv.2301.11975},
  abstractnote = {When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better results and faster inference in generation and classification tasks. The source code is shared on Github, along with a companion website. Finally, BPE is directly implemented in MidiTok, allowing the reader to easily benefit from this method.},
  note         = {arXiv:2301.11975 [cs]},
  booktitle    = {Conference on Empirical Methods in Natural Language Processing},
  publisher    = {arXiv},
  author       = {Fradet, Nathan and Gutowski, Nicolas and Chhel, Fabien and Briot, Jean-Pierre},
  year         = {2023},
  month        = nov
}

@inproceedings{Bemman2024,
  author    = {Bemman, Brian and Christensen, Justin},
  title     = {Inner Metric Analysis as a Measure of Rhythmic Syncopation},
  booktitle = {Proceedings of the 25th International Society for Music Information Retrieval Conference (ISMIR)},
  year      = {2024},
  month     = {11},
  address   = {San Francisco, USA},
  pages     = {389--396},
  url       = {https://ismir2024program.ismir.net/poster_251.html}
}

@inproceedings{language_guide_rutte_2024,
  author    = {von R\"{u}tte, Dimitri and Anagnostidis, Sotiris and Bachmann, Gregor and Hofmann, Thomas},
  title     = {A language model's guide through latent space},
  year      = {2024},
  publisher = {JMLR.org},
  abstract  = {Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on truthfulness, in this paper we extend this framework to a richer set of concepts such as appropriateness, humor, creativity and quality, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as truthfulness more easily allow for guidance with current techniques, novel concepts such as appropriateness or humor either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for truthfulness. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches.},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  articleno = {2030},
  numpages  = {33},
  location  = {Vienna, Austria},
  series    = {ICML'24}
}

@article{Volk2008Syncopation,
  author    = {Volk, Anja},
  title     = {The Study of Syncopation Using Inner Metric Analysis: Linking Theoretical and Experimental Analysis of Metre in Music},
  journal   = {Journal of New Music Research},
  year      = {2008},
  volume    = {37},
  number    = {4},
  pages     = {259--273},
  doi       = {10.1080/09298210802479213},
  url       = {https://webspace.science.uu.nl/~veltk101/publications/art/JNMR08.pdf}
}
